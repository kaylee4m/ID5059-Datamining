{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error,accuracy_score \n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "# %matplotlib inline\n",
    "\n",
    "np.random.seed(seed=2019)\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Member</th>\n",
       "      <th>Constituency</th>\n",
       "      <th>Party</th>\n",
       "      <th>Vote386</th>\n",
       "      <th>Vote387</th>\n",
       "      <th>Vote388</th>\n",
       "      <th>Vote389</th>\n",
       "      <th>Vote390</th>\n",
       "      <th>Vote391</th>\n",
       "      <th>Vote392</th>\n",
       "      <th>Vote393</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Diane Abbott</td>\n",
       "      <td>Hackney North and Stoke Newington</td>\n",
       "      <td>Lab</td>\n",
       "      <td>No</td>\n",
       "      <td>Aye</td>\n",
       "      <td>No Vote</td>\n",
       "      <td>Aye</td>\n",
       "      <td>Aye</td>\n",
       "      <td>No Vote</td>\n",
       "      <td>Aye</td>\n",
       "      <td>No</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Debbie Abrahams</td>\n",
       "      <td>Oldham East and Saddleworth</td>\n",
       "      <td>Lab</td>\n",
       "      <td>No</td>\n",
       "      <td>No Vote</td>\n",
       "      <td>No</td>\n",
       "      <td>Aye</td>\n",
       "      <td>Aye</td>\n",
       "      <td>No Vote</td>\n",
       "      <td>Aye</td>\n",
       "      <td>No</td>\n",
       "      <td>0.599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Nigel Adams</td>\n",
       "      <td>Selby and Ainsty</td>\n",
       "      <td>Con</td>\n",
       "      <td>No Vote</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Aye</td>\n",
       "      <td>0.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Adam Afriyie</td>\n",
       "      <td>Windsor</td>\n",
       "      <td>Con</td>\n",
       "      <td>Aye</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Aye</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Peter Aldous</td>\n",
       "      <td>Waveney</td>\n",
       "      <td>Con</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Aye</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           Member                       Constituency Party  \\\n",
       "0           1     Diane Abbott  Hackney North and Stoke Newington   Lab   \n",
       "1           2  Debbie Abrahams        Oldham East and Saddleworth   Lab   \n",
       "2           3      Nigel Adams                   Selby and Ainsty   Con   \n",
       "3           5     Adam Afriyie                            Windsor   Con   \n",
       "4           6     Peter Aldous                            Waveney   Con   \n",
       "\n",
       "   Vote386  Vote387  Vote388 Vote389 Vote390  Vote391 Vote392 Vote393  \\\n",
       "0       No      Aye  No Vote     Aye     Aye  No Vote     Aye      No   \n",
       "1       No  No Vote       No     Aye     Aye  No Vote     Aye      No   \n",
       "2  No Vote       No       No      No      No       No      No     Aye   \n",
       "3      Aye       No       No      No      No       No      No     Aye   \n",
       "4       No       No      Aye      No      No       No      No      No   \n",
       "\n",
       "   Percentage  \n",
       "0       0.205  \n",
       "1       0.599  \n",
       "2       0.577  \n",
       "3       0.467  \n",
       "4       0.634  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = pd.read_csv('Brexit_20190327_G04_traindata.csv')\n",
    "aaa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv('Brexit_20190327_G04_traindata.csv').drop(['Member','Unnamed: 0','Constituency','Party'], axis=1)\n",
    "\n",
    "raw_train.loc[raw_train['Percentage'] >= 0.4395,'Percentage'] = 1\n",
    "raw_train.loc[raw_train['Percentage'] < 0.4395,'Percentage'] = 0\n",
    "\n",
    "y_train = raw_train['Percentage']\n",
    "X_train = raw_train.drop(['Percentage'], axis=1)\n",
    "X_train.head()\n",
    "\n",
    "\n",
    "\n",
    "raw_test = pd.read_csv('Brexit_20190327_G04_testdata.csv').drop(['Member','Unnamed: 0','Constituency','Party'], axis=1)\n",
    "\n",
    "raw_test.loc[raw_test['Percentage'] >= 0.4395,'Percentage'] = 1\n",
    "raw_test.loc[raw_test['Percentage'] < 0.4395,'Percentage'] = 0\n",
    "\n",
    "y_test = raw_test['Percentage']\n",
    "X_test = raw_test.drop(['Percentage'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "Name: Percentage, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing \n",
    "for f in X_train.columns: \n",
    "    if X_train[f].dtype=='object': \n",
    "#         print(f)\n",
    "        lbl = preprocessing.LabelEncoder() \n",
    "        lbl.fit(list(X_train[f].values)) \n",
    "        X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "        \n",
    "for f in X_test.columns: \n",
    "    if X_test[f].dtype=='object':\n",
    "#         print(f)\n",
    "        lbl = preprocessing.LabelEncoder() \n",
    "        lbl.fit(list(X_test[f].values)) \n",
    "        X_test[f] = lbl.transform(list(X_test[f].values))\n",
    "y_test = pd.to_numeric(y_test)\n",
    "y_train= pd.to_numeric(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vote386</th>\n",
       "      <th>Vote387</th>\n",
       "      <th>Vote388</th>\n",
       "      <th>Vote389</th>\n",
       "      <th>Vote390</th>\n",
       "      <th>Vote391</th>\n",
       "      <th>Vote392</th>\n",
       "      <th>Vote393</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vote386  Vote387  Vote388  Vote389  Vote390  Vote391  Vote392  Vote393\n",
       "0        1        0        2        0        0        2        0        1\n",
       "1        1        2        1        0        0        2        0        1\n",
       "2        2        1        1        1        1        1        1        0\n",
       "3        0        1        1        1        1        1        1        0\n",
       "4        1        1        0        1        1        1        1        1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "    \n",
    "    def loss(self, coef, X, y):\n",
    "        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1])\n",
    "        return -accuracy_score(y, preds)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self.loss, X = X, y = y)\n",
    "        initial_coef = [0.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "    \n",
    "    def predict(self, X, coef):\n",
    "        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1])\n",
    "        return preds\n",
    "    \n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LGB:\n",
      "[1000]\ttraining's rmse: 0.31656\tvalid_1's rmse: 0.3225\n",
      "[2000]\ttraining's rmse: 0.308514\tvalid_1's rmse: 0.327464\n",
      "[3000]\ttraining's rmse: 0.304477\tvalid_1's rmse: 0.330616\n",
      "[4000]\ttraining's rmse: 0.301986\tvalid_1's rmse: 0.330228\n",
      "[5000]\ttraining's rmse: 0.299979\tvalid_1's rmse: 0.331771\n",
      "[6000]\ttraining's rmse: 0.298252\tvalid_1's rmse: 0.334051\n",
      "[7000]\ttraining's rmse: 0.296732\tvalid_1's rmse: 0.334935\n",
      "[8000]\ttraining's rmse: 0.295463\tvalid_1's rmse: 0.335686\n",
      "[9000]\ttraining's rmse: 0.294408\tvalid_1's rmse: 0.33582\n",
      "[10000]\ttraining's rmse: 0.293444\tvalid_1's rmse: 0.337033\n",
      "acc of lgb:\n",
      "0.7593582887700535\n",
      "training LGB:\n",
      "[1000]\ttraining's rmse: 0.315236\tvalid_1's rmse: 0.335131\n",
      "[2000]\ttraining's rmse: 0.308457\tvalid_1's rmse: 0.337275\n",
      "[3000]\ttraining's rmse: 0.305004\tvalid_1's rmse: 0.339916\n",
      "[4000]\ttraining's rmse: 0.302408\tvalid_1's rmse: 0.341428\n",
      "[5000]\ttraining's rmse: 0.300075\tvalid_1's rmse: 0.34356\n",
      "[6000]\ttraining's rmse: 0.298315\tvalid_1's rmse: 0.345366\n",
      "[7000]\ttraining's rmse: 0.296646\tvalid_1's rmse: 0.348884\n",
      "[8000]\ttraining's rmse: 0.29526\tvalid_1's rmse: 0.353044\n",
      "[9000]\ttraining's rmse: 0.294179\tvalid_1's rmse: 0.356511\n",
      "[10000]\ttraining's rmse: 0.293327\tvalid_1's rmse: 0.358693\n",
      "acc of lgb:\n",
      "0.786096256684492\n",
      "training LGB:\n",
      "[1000]\ttraining's rmse: 0.309707\tvalid_1's rmse: 0.410985\n",
      "[2000]\ttraining's rmse: 0.301889\tvalid_1's rmse: 0.413859\n",
      "[3000]\ttraining's rmse: 0.298494\tvalid_1's rmse: 0.412294\n",
      "[4000]\ttraining's rmse: 0.295966\tvalid_1's rmse: 0.41005\n",
      "[5000]\ttraining's rmse: 0.294227\tvalid_1's rmse: 0.408016\n",
      "[6000]\ttraining's rmse: 0.293021\tvalid_1's rmse: 0.405499\n",
      "[7000]\ttraining's rmse: 0.291861\tvalid_1's rmse: 0.403889\n",
      "[8000]\ttraining's rmse: 0.290968\tvalid_1's rmse: 0.402265\n",
      "[9000]\ttraining's rmse: 0.290231\tvalid_1's rmse: 0.401339\n",
      "[10000]\ttraining's rmse: 0.289551\tvalid_1's rmse: 0.400865\n",
      "acc of lgb:\n",
      "0.7967914438502673\n",
      "training LGB:\n",
      "[1000]\ttraining's rmse: 0.318654\tvalid_1's rmse: 0.311449\n",
      "[2000]\ttraining's rmse: 0.311103\tvalid_1's rmse: 0.313185\n",
      "[3000]\ttraining's rmse: 0.307756\tvalid_1's rmse: 0.317889\n",
      "[4000]\ttraining's rmse: 0.305352\tvalid_1's rmse: 0.318426\n",
      "[5000]\ttraining's rmse: 0.303648\tvalid_1's rmse: 0.321214\n",
      "[6000]\ttraining's rmse: 0.302318\tvalid_1's rmse: 0.322418\n",
      "[7000]\ttraining's rmse: 0.301083\tvalid_1's rmse: 0.32404\n",
      "[8000]\ttraining's rmse: 0.30005\tvalid_1's rmse: 0.325821\n",
      "[9000]\ttraining's rmse: 0.299155\tvalid_1's rmse: 0.326629\n",
      "[10000]\ttraining's rmse: 0.298442\tvalid_1's rmse: 0.326429\n",
      "acc of lgb:\n",
      "0.786096256684492\n",
      "training LGB:\n",
      "[1000]\ttraining's rmse: 0.318081\tvalid_1's rmse: 0.313428\n",
      "[2000]\ttraining's rmse: 0.311003\tvalid_1's rmse: 0.313655\n",
      "[3000]\ttraining's rmse: 0.307459\tvalid_1's rmse: 0.316603\n",
      "[4000]\ttraining's rmse: 0.305078\tvalid_1's rmse: 0.319838\n",
      "[5000]\ttraining's rmse: 0.303198\tvalid_1's rmse: 0.320893\n",
      "[6000]\ttraining's rmse: 0.301494\tvalid_1's rmse: 0.320563\n",
      "[7000]\ttraining's rmse: 0.300107\tvalid_1's rmse: 0.321072\n",
      "[8000]\ttraining's rmse: 0.298829\tvalid_1's rmse: 0.321283\n",
      "[9000]\ttraining's rmse: 0.297712\tvalid_1's rmse: 0.321775\n",
      "[10000]\ttraining's rmse: 0.296758\tvalid_1's rmse: 0.322368\n",
      "acc of lgb:\n",
      "0.786096256684492\n",
      "training LGB:\n",
      "[1000]\ttraining's rmse: 0.314241\tvalid_1's rmse: 0.377303\n",
      "[2000]\ttraining's rmse: 0.307003\tvalid_1's rmse: 0.367679\n",
      "[3000]\ttraining's rmse: 0.302751\tvalid_1's rmse: 0.366662\n",
      "[4000]\ttraining's rmse: 0.299912\tvalid_1's rmse: 0.367638\n",
      "[5000]\ttraining's rmse: 0.297424\tvalid_1's rmse: 0.369734\n",
      "[6000]\ttraining's rmse: 0.295497\tvalid_1's rmse: 0.369892\n",
      "[7000]\ttraining's rmse: 0.293979\tvalid_1's rmse: 0.369776\n",
      "[8000]\ttraining's rmse: 0.29225\tvalid_1's rmse: 0.371081\n",
      "[9000]\ttraining's rmse: 0.291075\tvalid_1's rmse: 0.372018\n",
      "[10000]\ttraining's rmse: 0.29028\tvalid_1's rmse: 0.373587\n",
      "acc of lgb:\n",
      "0.7700534759358288\n",
      "training LGB:\n",
      "[1000]\ttraining's rmse: 0.325574\tvalid_1's rmse: 0.26602\n",
      "[2000]\ttraining's rmse: 0.318052\tvalid_1's rmse: 0.259353\n",
      "[3000]\ttraining's rmse: 0.31451\tvalid_1's rmse: 0.255447\n",
      "[4000]\ttraining's rmse: 0.312493\tvalid_1's rmse: 0.252226\n",
      "[5000]\ttraining's rmse: 0.31081\tvalid_1's rmse: 0.250614\n",
      "[6000]\ttraining's rmse: 0.309474\tvalid_1's rmse: 0.249637\n",
      "[7000]\ttraining's rmse: 0.308169\tvalid_1's rmse: 0.248978\n",
      "[8000]\ttraining's rmse: 0.307085\tvalid_1's rmse: 0.248043\n",
      "[9000]\ttraining's rmse: 0.306205\tvalid_1's rmse: 0.247529\n",
      "[10000]\ttraining's rmse: 0.305546\tvalid_1's rmse: 0.247134\n",
      "acc of lgb:\n",
      "0.8021390374331551\n",
      "training LGB:\n",
      "[1000]\ttraining's rmse: 0.320568\tvalid_1's rmse: 0.309765\n",
      "[2000]\ttraining's rmse: 0.310833\tvalid_1's rmse: 0.321823\n",
      "[3000]\ttraining's rmse: 0.307609\tvalid_1's rmse: 0.322538\n",
      "[4000]\ttraining's rmse: 0.304869\tvalid_1's rmse: 0.325353\n",
      "[5000]\ttraining's rmse: 0.302272\tvalid_1's rmse: 0.328331\n",
      "[6000]\ttraining's rmse: 0.300508\tvalid_1's rmse: 0.327952\n",
      "[7000]\ttraining's rmse: 0.299264\tvalid_1's rmse: 0.328436\n",
      "[8000]\ttraining's rmse: 0.298274\tvalid_1's rmse: 0.329007\n",
      "[9000]\ttraining's rmse: 0.297041\tvalid_1's rmse: 0.329866\n",
      "[10000]\ttraining's rmse: 0.296133\tvalid_1's rmse: 0.331434\n",
      "acc of lgb:\n",
      "0.8074866310160428\n",
      "training LGB:\n",
      "[1000]\ttraining's rmse: 0.319503\tvalid_1's rmse: 0.323536\n",
      "[2000]\ttraining's rmse: 0.311678\tvalid_1's rmse: 0.316783\n",
      "[3000]\ttraining's rmse: 0.307576\tvalid_1's rmse: 0.316598\n",
      "[4000]\ttraining's rmse: 0.304331\tvalid_1's rmse: 0.317032\n",
      "[5000]\ttraining's rmse: 0.301801\tvalid_1's rmse: 0.319163\n",
      "[6000]\ttraining's rmse: 0.299965\tvalid_1's rmse: 0.321407\n",
      "[7000]\ttraining's rmse: 0.298594\tvalid_1's rmse: 0.323028\n",
      "[8000]\ttraining's rmse: 0.297369\tvalid_1's rmse: 0.325199\n",
      "[9000]\ttraining's rmse: 0.296311\tvalid_1's rmse: 0.327916\n",
      "[10000]\ttraining's rmse: 0.295413\tvalid_1's rmse: 0.329537\n",
      "acc of lgb:\n",
      "0.7914438502673797\n",
      "training LGB:\n",
      "[1000]\ttraining's rmse: 0.317192\tvalid_1's rmse: 0.337465\n",
      "[2000]\ttraining's rmse: 0.309883\tvalid_1's rmse: 0.333342\n",
      "[3000]\ttraining's rmse: 0.305655\tvalid_1's rmse: 0.332219\n",
      "[4000]\ttraining's rmse: 0.303079\tvalid_1's rmse: 0.331641\n",
      "[5000]\ttraining's rmse: 0.301007\tvalid_1's rmse: 0.331025\n",
      "[6000]\ttraining's rmse: 0.299405\tvalid_1's rmse: 0.330954\n",
      "[7000]\ttraining's rmse: 0.298381\tvalid_1's rmse: 0.330774\n",
      "[8000]\ttraining's rmse: 0.297475\tvalid_1's rmse: 0.330549\n",
      "[9000]\ttraining's rmse: 0.296779\tvalid_1's rmse: 0.33\n",
      "[10000]\ttraining's rmse: 0.296218\tvalid_1's rmse: 0.329573\n",
      "acc of lgb:\n",
      "0.786096256684492\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "\n",
    "xgb_params = {\n",
    "#     'objective': 'binary:logistic',\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 2019,\n",
    "    'eta': 0.0123,\n",
    "#     'subsample': 0.8,\n",
    "#     'colsample_bytree': 0.85,\n",
    "    'tree_method': 'hist',\n",
    "    'silent': 1,\n",
    "    \n",
    "}\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "#           'max_depth': 9,\n",
    "          'learning_rate': 0.004,\n",
    "#           'bagging_fraction': 0.85,\n",
    "#           'feature_fraction': 0.8,\n",
    "#           'min_split_gain': 0.02,\n",
    "#           'min_child_samples': 150,\n",
    "#           'min_child_weight': 0.02,\n",
    "#           'lambda_l2': 0.0475,\n",
    "#           'verbosity': -1,\n",
    "          'data_random_seed': 17\n",
    "    \n",
    "}\n",
    "\n",
    "def run_xlgb(params1, params2, X_train, X_test,y_train,y_test):\n",
    "    n_splits = 10\n",
    "    verbose_eval = 1000\n",
    "    num_rounds = 10000\n",
    "    early_stop = 20\n",
    "\n",
    "    kf =StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2019)\n",
    "\n",
    "    oof_train = np.zeros((X_train.shape[0]))\n",
    "    oof_test = np.zeros((X_test.shape[0], n_splits),dtype=np.int8)\n",
    "   \n",
    "    i = 0\n",
    "\n",
    "    for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "#         print(train_idx)\n",
    "        X_tr = X_train.iloc[train_idx, :]\n",
    "        X_val = X_train.iloc[valid_idx, :]\n",
    "        \n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        y_val = y_train.iloc[valid_idx]\n",
    "\n",
    "#         d_train1 = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n",
    "#         d_valid1 = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n",
    "\n",
    "#         watchlist1 = [(d_train1, 'train'), (d_valid1, 'valid')]\n",
    "#         print('training XGB:')\n",
    "#         model1 = xgb.train(dtrain=d_train1, num_boost_round=num_rounds, evals=watchlist1,early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params1)\n",
    "\n",
    "#         valid_pred1 = model1.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model1.best_ntree_limit)\n",
    "#         test_pred1 = model1.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model1.best_ntree_limit)\n",
    "#         print('acc of xgb:')\n",
    "#         print(accuracy_score(y_test, np.round(test_pred1).astype(np.int8)))\n",
    "# #         print(test_pred1)\n",
    "#         oof_train[valid_idx] = valid_pred1\n",
    "#         oof_test[:, i] = np.round(test_pred1)\n",
    "\n",
    "        ####################\n",
    "\n",
    "\n",
    "        d_train2 = lgb.Dataset(X_tr, label=y_tr)\n",
    "        d_valid2 = lgb.Dataset(X_val, label=y_val)\n",
    "#         lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "        watchlist2 = [d_train2, d_valid2]\n",
    "        print('training LGB:')\n",
    "                  \n",
    "        model2 = lgb.train(params2,\n",
    "                      train_set=d_train2,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist2,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      )\n",
    "\n",
    "        valid_pred2 = model2.predict(X_val, num_iteration=model2.best_iteration)\n",
    "        test_pred2 = model2.predict(X_test, num_iteration=model2.best_iteration)\n",
    "\n",
    "        oof_train[valid_idx] = valid_pred2\n",
    "        oof_test[:, i] = np.round(test_pred2)\n",
    "        print('acc of lgb:')\n",
    "        print(accuracy_score(y_test, np.round(test_pred2).astype(np.int8)))\n",
    "        \n",
    "        i += 1\n",
    "    return oof_train, oof_test\n",
    "\n",
    "oof_train, oof_test = run_xlgb(xgb_params,lgb_params, X_train, X_test,y_train,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8497757847533632\n",
      "0.786096256684492\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import scipy as sp\n",
    "\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error,accuracy_score \n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(oof_train, y_train.values)\n",
    "coefficients = optR.coefficients()\n",
    "valid_pred = optR.predict(oof_train, coefficients)\n",
    "acc_val = accuracy_score(y_train, valid_pred)\n",
    "print(acc_val)\n",
    "\n",
    "\n",
    "test_predictions = optR.predict(oof_test.mean(axis=1), coefficients).astype(np.int8)\n",
    "acc_test = accuracy_score(y_test, test_predictions)\n",
    "print(acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48125])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1], dtype=int8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test pred distribution: Counter({1: 163, 0: 24})\n"
     ]
    }
   ],
   "source": [
    "print(f'test pred distribution: {Counter(test_predictions)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.496199\ttest-rmse:0.496927\n",
      "Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until test-rmse hasn't improved in 20 rounds.\n",
      "[100]\ttrain-rmse:0.320019\ttest-rmse:0.382729\n",
      "Stopping. Best iteration:\n",
      "[132]\ttrain-rmse:0.305246\ttest-rmse:0.380561\n",
      "\n",
      "acc of xgb:\n",
      "0.7754010695187166\n"
     ]
    }
   ],
   "source": [
    "### XGB ### \n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_params = {\n",
    "#     'objective': 'binary:logistic',\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 2019,\n",
    "    'eta': 0.0123,\n",
    "#     'subsample': 0.8,\n",
    "#     'colsample_bytree': 0.85,\n",
    "    'tree_method': 'hist',\n",
    "    'silent': 1,\n",
    "    \n",
    "}\n",
    "\n",
    "xgb_train = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n",
    "xgb_test = xgb.DMatrix(data=X_test, label=y_test, feature_names=X_test.columns)\n",
    "\n",
    "watchlist1 = [(xgb_train, 'train'), (xgb_test, 'test')]\n",
    "model_xgb = xgb.train(dtrain=xgb_train, num_boost_round=10000, evals=watchlist1,early_stopping_rounds=20, verbose_eval=100, params=xgb_params)\n",
    "# valid_pred1 = model1.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model1.best_ntree_limit)\n",
    "test_pred_xgb = model_xgb.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model_xgb.best_ntree_limit)\n",
    "print('acc of xgb:')\n",
    "print(accuracy_score(y_test, np.round(test_pred_xgb).astype(np.int8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496249</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.496609</td>\n",
       "      <td>0.000391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.492551</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.000789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.488918</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.490023</td>\n",
       "      <td>0.001176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.485338</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.486827</td>\n",
       "      <td>0.001574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.481823</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.483673</td>\n",
       "      <td>0.001961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.478350</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.480606</td>\n",
       "      <td>0.002352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.474944</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.477582</td>\n",
       "      <td>0.002735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.471588</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.474629</td>\n",
       "      <td>0.003125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.468293</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.471713</td>\n",
       "      <td>0.003504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.465050</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.468882</td>\n",
       "      <td>0.003894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.461865</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.466083</td>\n",
       "      <td>0.004269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.458736</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.463353</td>\n",
       "      <td>0.004640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.455660</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.460680</td>\n",
       "      <td>0.005022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.452630</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.458065</td>\n",
       "      <td>0.005395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.449647</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.455488</td>\n",
       "      <td>0.005767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.446714</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.452953</td>\n",
       "      <td>0.006125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.443823</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.450510</td>\n",
       "      <td>0.006501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.440987</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.448087</td>\n",
       "      <td>0.006855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.438200</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.445741</td>\n",
       "      <td>0.007233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.435463</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.443450</td>\n",
       "      <td>0.007597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.432767</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.441183</td>\n",
       "      <td>0.007969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.430089</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.439001</td>\n",
       "      <td>0.008366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.427459</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.436853</td>\n",
       "      <td>0.008727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.424881</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.434800</td>\n",
       "      <td>0.009112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.422347</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.432734</td>\n",
       "      <td>0.009474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.419840</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.430716</td>\n",
       "      <td>0.009854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.417385</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.428733</td>\n",
       "      <td>0.010226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.414967</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.426820</td>\n",
       "      <td>0.010592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.412611</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.424913</td>\n",
       "      <td>0.010951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.410297</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.423096</td>\n",
       "      <td>0.011284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.049029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0            0.496249        0.000069        0.496609       0.000391\n",
       "1            0.492551        0.000137        0.493300       0.000789\n",
       "2            0.488918        0.000209        0.490023       0.001176\n",
       "3            0.485338        0.000281        0.486827       0.001574\n",
       "4            0.481823        0.000352        0.483673       0.001961\n",
       "5            0.478350        0.000406        0.480606       0.002352\n",
       "6            0.474944        0.000450        0.477582       0.002735\n",
       "7            0.471588        0.000506        0.474629       0.003125\n",
       "8            0.468293        0.000560        0.471713       0.003504\n",
       "9            0.465050        0.000619        0.468882       0.003894\n",
       "10           0.461865        0.000676        0.466083       0.004269\n",
       "11           0.458736        0.000735        0.463353       0.004640\n",
       "12           0.455660        0.000788        0.460680       0.005022\n",
       "13           0.452630        0.000838        0.458065       0.005395\n",
       "14           0.449647        0.000890        0.455488       0.005767\n",
       "15           0.446714        0.000934        0.452953       0.006125\n",
       "16           0.443823        0.000984        0.450510       0.006501\n",
       "17           0.440987        0.001028        0.448087       0.006855\n",
       "18           0.438200        0.001073        0.445741       0.007233\n",
       "19           0.435463        0.001122        0.443450       0.007597\n",
       "20           0.432767        0.001168        0.441183       0.007969\n",
       "21           0.430089        0.001202        0.439001       0.008366\n",
       "22           0.427459        0.001232        0.436853       0.008727\n",
       "23           0.424881        0.001265        0.434800       0.009112\n",
       "24           0.422347        0.001290        0.432734       0.009474\n",
       "25           0.419840        0.001318        0.430716       0.009854\n",
       "26           0.417385        0.001347        0.428733       0.010226\n",
       "27           0.414967        0.001383        0.426820       0.010592\n",
       "28           0.412611        0.001424        0.424913       0.010951\n",
       "29           0.410297        0.001451        0.423096       0.011284\n",
       "...               ...             ...             ...            ...\n",
       "9970         0.282732        0.005784        0.369152       0.049029\n",
       "9971         0.282732        0.005784        0.369152       0.049029\n",
       "9972         0.282732        0.005784        0.369152       0.049029\n",
       "9973         0.282732        0.005784        0.369152       0.049029\n",
       "9974         0.282732        0.005784        0.369152       0.049029\n",
       "9975         0.282732        0.005784        0.369152       0.049029\n",
       "9976         0.282732        0.005784        0.369152       0.049029\n",
       "9977         0.282732        0.005784        0.369152       0.049029\n",
       "9978         0.282732        0.005784        0.369152       0.049029\n",
       "9979         0.282732        0.005784        0.369152       0.049029\n",
       "9980         0.282732        0.005784        0.369152       0.049029\n",
       "9981         0.282732        0.005784        0.369152       0.049029\n",
       "9982         0.282732        0.005784        0.369152       0.049029\n",
       "9983         0.282732        0.005784        0.369152       0.049029\n",
       "9984         0.282732        0.005784        0.369152       0.049029\n",
       "9985         0.282732        0.005784        0.369152       0.049029\n",
       "9986         0.282732        0.005784        0.369152       0.049029\n",
       "9987         0.282732        0.005784        0.369152       0.049029\n",
       "9988         0.282732        0.005784        0.369152       0.049029\n",
       "9989         0.282732        0.005784        0.369152       0.049029\n",
       "9990         0.282732        0.005784        0.369152       0.049029\n",
       "9991         0.282732        0.005784        0.369152       0.049029\n",
       "9992         0.282732        0.005784        0.369152       0.049029\n",
       "9993         0.282732        0.005784        0.369152       0.049029\n",
       "9994         0.282732        0.005784        0.369152       0.049029\n",
       "9995         0.282732        0.005784        0.369152       0.049029\n",
       "9996         0.282732        0.005784        0.369152       0.049029\n",
       "9997         0.282732        0.005784        0.369152       0.049029\n",
       "9998         0.282732        0.005784        0.369152       0.049029\n",
       "9999         0.282732        0.005784        0.369152       0.049029\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.cv(xgb_params, dtrain=xgb_train, num_boost_round=10000, nfold=10, stratified=False, folds=None, metrics=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XuUVNWd9vHvr6tvXBqwAX0R0AYGk4CNzR3CMEui3ExA4xsnxplEJxONUcbk9R1HWEYURtckrlnGmBiNdyYm8YITRSGvBIVJxqjQJC2RmzQXhw5EWxAU6Gv17/2jTjfVTVNUQ0Ptpp/PWrWqzj6X2nU41NP77HN2mbsjIiJyNFmZroCIiIRNQSEiIikpKEREJCUFhYiIpKSgEBGRlBQUIiKSkoJCRERSUlCIiEhKCgoREUkpO9MVaKlPnz5eVFSU6WqIiHQoa9eu/dDd+56MbQcXFEVFRZSWlma6GiIiHYqZvXeytq1TTyIikpKCQkREUkorKMxshpltNrNyM5vbyvxrzKzSzMqixzeS5l1tZluix9XtWXkRETn5jtlHYWYx4AFgKlABrDGzJe6+ocWiz7j7nBbrFgJ3AGMAB9ZG637ULrUXkVbV1dVRUVFBdXV1pqsi7Sw/P58BAwaQk5Nzyt4znc7scUC5u28DMLOngUuBlkHRmunAb9x9b7Tub4AZwC+Pr7oiko6KigoKCgooKirCzDJdHWkn7s6ePXuoqKhg0KBBp+x90zn11B/YmTRdEZW19L/NbJ2ZLTazgW1Z18yuM7NSMyutrKxMs+oicjTV1dX07t1bIXGaMTN69+59yluK6QRFa0day5/FewkocvcRwApgURvWxd0fdvcx7j6mb9+TchmwSKejkDg9ZeLfNZ2gqAAGJk0PAHYlL+Due9y9Jpp8BBid7rotVe4/QFVNXRrVEhGRUyGdoFgDDDWzQWaWC1wJLElewMz6JU3OBjZGr18BppnZGWZ2BjAtKjuqvge3UP3JnnTrLyIB2rdvHz/5yU+Oa9377ruPQ4cOtXON5EQcMyjcvR6YQ+ILfiPwrLuvN7OFZjY7WuwmM1tvZm8DNwHXROvuBf6VRNisARY2dmynUh9Xi0KkIzsVQVFfX39c25e2S2sID3dfBixrUTY/6fU8YN5R1n0ceLwtlfL6hrYsLiKBmTt3Llu3bqWkpISpU6dy5pln8uyzz1JTU8MXv/hFFixYwMGDB/nbv/1bKioqiMfj3H777bz//vvs2rWLKVOm0KdPH1auXNlsu08++SRLly6lurqagwcPMn/+fO644w7OOussysrKuPzyyykuLuaHP/whVVVVvPDCCwwZMoTnnnuOBQsWEIvF6NmzJ7/97W+Jx+PMnTuXVatWUVNTw4033sg3v/nNIz7LNddcQ5cuXdi0aRPvvfceTzzxBIsWLeKNN95g/PjxPPnkkwAsX76cO+64g5qaGoYMGcITTzxB9+7dWbhwIS+99BJVVVV89rOf5ac//SlmxoUXXsj48eNZuXIl+/bt47HHHmPy5Mmn4p+nzYIb6wkg3qC/FETay4KX1rNh18ftus1hZ/fgjlnDjzr/e9/7Hu+88w5lZWUsX76cxYsXs3r1atyd2bNn89vf/pbKykrOPvtsli5dCsD+/fvp2bMn9957LytXrqRPnz6tbvuNN95g3bp1FBYWsmrVKt5++202btxIYWEhgwcP5hvf+AarV6/mhz/8IT/60Y+47777WLhwIa+88gr9+/dn3759ADz22GP07NmTNWvWUFNTw6RJk5g2bVqrl51+9NFHvPbaayxZsoRZs2bx+uuv8+ijjzJ27FjKysoYMGAAd911FytWrKBbt258//vf595772X+/PnMmTOH+fMTf1d/9atf5eWXX2bWrFlAolW0evVqli1bxoIFC1ixYsUJ/bucLEEO4dEQj2e6CiLSTpYvX87y5csZOXIko0aNYtOmTWzZsoXi4mJWrFjBrbfeyu9+9zt69uyZ1vamTp1KYWFh0/TYsWPp168feXl5DBkyhGnTpgFQXFzMjh07AJg0aRLXXHMNjzzyCPHo+2X58uX8x3/8ByUlJYwfP549e/awZcuWVt9z1qxZmBnFxcWcddZZFBcXk5WVxfDhw9mxYwdvvvkmGzZsYNKkSZSUlLBo0SLeey8xRt/KlSsZP348xcXFvPbaa6xfv75pu5dffjkAo0ePbqpriIJsUTQ0KChE2kuqv/xPBXdn3rx5rZ7WWbt2LcuWLWPevHlMmzat6S/vRr/61a9YsGABAI8++igA3bp1a7ZMXl5e0+usrKym6aysrKZ+jIceeoi33nqLpUuXUlJSQllZGe7Oj370I6ZPn95se7fddltTK6esrKzZeyRvP/k9YrEYU6dO5Ze/bH4vcXV1NTfccAOlpaUMHDiQO++8s9k9EI3bisViQfe5BNqiCHeHicixFRQU8MknnwAwffp0Hn/8cQ4cOADAn//8Zz744AN27dpF165d+fu//3v++Z//mT/84Q9HrPvFL36RsrIyysrKGDNmzHHXZ+vWrYwfP56FCxfSp08fdu7cyfTp03nwwQepq0tcPPPuu+9y8OBB7r777qb3TNeECRN4/fXXKS8vB+DQoUO8++67TaHQp08fDhw4wOLFi4/7M2RSkC2KuE49iXRovXv3ZtKkSZx//vnMnDmTq666iokTJwLQvXt3nnrqKcrLy7nlllvIysoiJyeHBx98EIDrrruOmTNn0q9fvyM6s4/XLbfcwpYtW3B3LrroIi644AJGjBjBjh07GDVqFO5O3759eeGFF45r+3379uXJJ5/kK1/5CjU1iVvK7rrrLs477zyuvfZaiouLKSoqYuzYse3yeU41cz/iRumMGnN2zJ/+zVv81fDj/+tBpLPbuHEjn/nMZzJdDTlJWvv3NbO17n5SvjiDPPXk6qMQEQlGkEGhPgoRkXAEGhRqUYiIhCLQoFCLQkQkFGEGhfooRESCEWRQqDNbRCQcQQaFTj2JdGzHO3rsJZdc0jQWk4QjyKBQi0KkYztaUBzrZtply5bRq1ev43pPd6ehQSNPnwxhBoWuehLp0JKHGR87dixTpkzhqquuori4GIDLLruM0aNHM3z4cB5++OGm9YqKivjwww/ZsWMHn/nMZ7j22msZPnw406ZNo6qq6oj3aVzuhhtuYNSoUezcuZPu3btz6623Mnr0aC6++GJWr17NhRdeyODBg1myJPGba+vXr2fcuHGUlJQwYsSIpsEAn3rqqabyb37zm60G25NPPslll13GrFmzGDRoED/+8Y+59957GTlyJBMmTGDv3sRP7mzdupUZM2YwevRoJk+ezKZNmwB46aWXGD9+PCNHjuTiiy/m/fffB+DOO+/k61//elNd77///nb8FzlB7h7UY3S/LF/zm2dcRI7fhg0bDk8su9X98Uva97Hs1pTvv337dh8+fLi7u69cudK7du3q27Zta5q/Z88ed3c/dOiQDx8+3D/88EN3dz/33HO9srLSt2/f7rFYzP/4xz+6u/sVV1zhP/vZz1p9HzPzN954o6kM8GXLlrm7+2WXXeZTp0712tpaLysr8wsuuMDd3efMmeNPPfWUu7vX1NT4oUOHfMOGDf6FL3zBa2tr3d39W9/6li9atOiI93ziiSd8yJAh/vHHH/sHH3zgPXr08AcffNDd3b/zne/4D37wA3d3/9znPufvvvuuu7u/+eabPmXKFHd337t3rzc0NLi7+yOPPOI333yzu7vfcccdPnHiRK+urvbKykovLCxsqktLzf59D3/uUj9J38tBjvXkaj6KnFbGjRvX7Hce7r//fn71q18BsHPnTrZs2ULv3r2brTNo0CBKSkqA1MNwn3vuuUyYMKFpOjc3lxkzZgCJocbz8vLIyclpNuz4xIkTufvuu6moqODyyy9n6NChvPrqq6xdu7ZpPKaqqirOPPPMVt9zypQpFBQUUFBQQM+ePZt+X6K4uJh169Zx4MABfv/733PFFVc0rdM4BlRFRQVf/vKX2b17N7W1tc32y+c//3ny8vLIy8vjzDPP5P3332fAgAGpd+4pEGhQ6NSTSLuZ+b1M16DZ0OCrVq1ixYoVvPHGG3Tt2pULL7yw2dDbjZKH847FYlRVVbFz586mL+Xrr7+eGTNmHDHseE5ODmYGHH3Y8auuuorx48ezdOlSpk+fzqOPPoq7c/XVV/Nv//ZvzbbX2lDnxxravKGhgV69erU6Au0//dM/cfPNNzN79mxWrVrFnXfeedTPHMrQ42H2UegX7kQ6tOShwlvav38/Z5xxBl27dmXTpk28+eabaW934MCBTUOAX3/99cddv23btjF48GBuuukmZs+ezbp167joootYvHgxH3zwAQB79+7lvffeO66hznv06MGgQYN47rnngMQp/rfffhtIfP7+/fsDsGjRouP+DKdSkEGhITxEOrbkYcZvueWWZvNmzJhBfX09I0aM4Pbbb2922uhUeeaZZzj//PMpKSlh06ZNfO1rX2PYsGHcddddTJs2jREjRjB16lR279593O/x85//nMcee4wLLriA4cOH8+KLLwKJTusrrriCyZMnH/XnXkMT5DDj9z/4EJ+99NpMV0Wkw9Iw46c3DTMO4GpRiIiEIsigUGe2iEg4wgwK9VGInLDQTitL+8jEv2uYQaEWhcgJyc/PZ8+ePQqL04y7s2fPHvLz80/p+wZ5H4X6KEROzIABA6ioqKCysjLTVZF2lp+ff8pvwgsyKNSiEDkxOTk5ze74FTkROvUkIiIpBRkUKChERIIRZFCoRSEiEo4ggwLX6LEiIqEIMihcVz2JiAQjyKAwnXoSEQlGkEGhzmwRkXAEGRQ69SQiEo4gg8IUFCIiwQguKBzTqScRkYCkFRRmNsPMNptZuZnNTbHcl8zMzWxMNF1kZlVmVhY9HkqrVro8VkQkGMcc68nMYsADwFSgAlhjZkvcfUOL5QqAm4C3Wmxiq7uXpF8ltShEREKSTotiHFDu7tvcvRZ4Gri0leX+FbgHqD6RCjlo9FgRkYCkExT9gZ1J0xVRWRMzGwkMdPeXW1l/kJn90cz+y8wmt/YGZnadmZWaWSmA6dSTiEgw0hlm3Fopa/o1FDPLAn4AXNPKcruBc9x9j5mNBl4ws+Hu/nGzjbk/DDwMMPLsXNepJxGRcKTToqgABiZNDwB2JU0XAOcDq8xsBzABWGJmY9y9xt33ALj7WmArcN6x3lCXx4qIhCOdoFgDDDWzQWaWC1wJLGmc6e773b2Puxe5exHwJjDb3UvNrG/UGY6ZDQaGAttSvZljuupJRCQgxzz15O71ZjYHeAWIAY+7+3ozWwiUuvuSFKv/DbDQzOqBOHC9u+891nuqRSEiEo60fgrV3ZcBy1qUzT/KshcmvX4eeL5NNTK1KEREQhLgndkoKEREAhJcUICGGRcRCUmAQWG64U5EJCABBoU6s0VEQhJcUOjyWBGRsAQXFKAWhYhISMILCjON9SQiEpDwggK1KEREQhJgUKhFISISkgCDAgy1KEREQhFeUKiPQkQkKMEFhWNkKShERIIRXFCATj2JiIQkwKBQi0JEJCThBYVBjDgNDX7sZUVE5KQLLygwsmmgrkGtChGREAQaFHHq42pRiIiEILygMCOmoBARCUZ4QRG1KHTqSUQkDOEFhRkxa1CLQkQkEOEFBSRaFHG1KEREQhBeUJgpKEREAhJeUDRe9aT7KEREghBeUJgRo0EtChGRQAQXFKb7KEREghJcUDS2KOp1eayISBDCCwogx+LU1SsoRERCEF5QmAFQX1+f4YqIiAgEGBQWBUU8XpfhmoiICAQYFBAFRZ2CQkQkBOEFRWOLol5BISISguCCwpr6KGozXBMREYEAg6Lx1FO9Tj2JiAQhuKA43KJQUIiIhCDYoKhTi0JEJAjBBkV9nfooRERCEG5QqDNbRCQIaQWFmc0ws81mVm5mc1Ms9yUzczMbk1Q2L1pvs5lNT+PNAHVmi4iEIvtYC5hZDHgAmApUAGvMbIm7b2ixXAFwE/BWUtkw4EpgOHA2sMLMznP3eIp3BDSEh4hIKNJpUYwDyt19m7vXAk8Dl7ay3L8C9wDVSWWXAk+7e427bwfKo+2l0HjDnU49iYiEIJ2g6A/sTJquiMqamNlIYKC7v9zWdY+gO7NFRIKSTlBYK2VNvypkZlnAD4D/29Z1k7ZxnZmVmlnp/v37AWhQUIiIBCGdoKgABiZNDwB2JU0XAOcDq8xsBzABWBJ1aB9rXQDc/WF3H+PuY3r26gWoRSEiEop0gmINMNTMBplZLonO6SWNM919v7v3cfcidy8C3gRmu3tptNyVZpZnZoOAocDq1G+nYcZFREJyzKue3L3ezOYArwAx4HF3X29mC4FSd1+SYt31ZvYssAGoB25MfcUTTX0UDbrqSUQkCMcMCgB3XwYsa1E2/yjLXthi+m7g7vSrFAVFXFc9iYiEILg7sw8HhVoUIiIhCC8oouukFBQiImEILyiipHB1ZouIBCG8oLDGoFCLQkQkBOEFRVMfhVoUIiIhCC8oki6PdT/iJm4RETnFwguKiHmc2nhDpqshItLphRcUUYsimzgHa1LfmyciIidfeEFBclCoQ1tEJNOCDopDtWpRiIhkWnhBkXzqqVYtChGRTAsvKIAGyybH6nXqSUQkAEEGBbEccqlXZ7aISACCDAqP5ZJDPYd06klEJOOCDApralEoKEREMi3IoCA7jxzqOairnkREMi7IoLBYLrlWzyG1KEREMi7YoMjPiqtFISISgCCDglguXbIa2F+lEWRFRDIt0KDIoWsszt6D+t1sEZFMCzQocukSa2CPgkJEJOMCDYoc8rPi7D1Yk+maiIh0eoEGRS75Vs/eA2pRiIhkWrBBkWeJ+yiq63Tlk4hIJgUaFDnkWCIg1KEtIpJZgQZFYqwngD06/SQiklFhBkU0hAfArv1VGa6MiEjnlp3pCrQqlkO2J26227n3UEarUhdv4GBNPQdqEsOeJ54TI9seqIknzavnUG2ceIMTd6ehwYk3eEbrLh1X9PtdIkEINChysYY6CvKz+Z82BEVtfQPbPzxIXbyBXl1zqK5r4EBNPQeq6zlQU8cn1fV8Ul1PVV2cQ7XNv/gPRF/0yV/8B2vi1MYb0qtyltE1J0Z2zIhlGVmWeNb/d2kr/XkhbeUn+aAJNyjidZxT2DWtFkXFR4e45/9t5tfv7KYunt4ei2UZ3XJjdM/LplvTI0bvbl2bl+XG6JaXnVQWi8obyxLTedlZmP4MFJEMsdtO3rYDDYociNdyTmFXNv3lk6MudqCmngdXlfPo77YD8Hfjz2XkOb3Iy87i46p68nKyKMjPpnteDt3zsqPX2XTNi5Eb0xe7iEg6Ag2KXIjXMux/FfDrd/7C/qo6enbJabbIuop93PiLP7BzbxWXlZzNv8z4NGf36pKhCouInL4CDYpEKIw+pwCA18s/5JLifk2zn12zk+++8A69u+fy3PUTGVtUmJFqioh0BmFeHhvLBWDcwG4MLOzCI7/bhrtTXRdn/ovv8C/Pr2P84EJ+/e3JCgkRkZMs0BZFIiiyvZ5rJw9m/ovruWvpRl7d+D479hziG389iLkzP012LMycExE5nYT5TRudeiJexxWjBzJiQE8e++/tZMey+Nk/juO7XximkBAROUUCbVHkJZ7jtXTJjfHijZOoPFBDn255ZGXpSiURkVMprT/LzWyGmW02s3Izm9vK/OvN7E9mVmZm/21mw6LyIjOrisrLzOyhtGoVnXoiXtu4fc4syFdIiIhkwDFbFGYWAx4ApgIVwBozW+LuG5IW+4W7PxQtPxu4F5gRzdvq7iVtq1UUFPX64SIRkUxLp0UxDih3923uXgs8DVyavIC7f5w02Y0THYUgO7ofol4DAoqIZFo6QdEf2Jk0XRGVNWNmN5rZVuAe4KakWYPM7I9m9l9mNjmtWmVHfRR11WktLiIiJ086QdFax8ARLQZ3f8DdhwC3At+NincD57j7SOBm4Bdm1uOINzC7zsxKzay0srISchpbFAoKEZFMSycoKoCBSdMDgF0pln8auAzA3WvcfU/0ei2wFTiv5Qru/rC7j3H3MX379oXs/MQMBYWISMalExRrgKFmNsjMcoErgSXJC5jZ0KTJzwNbovK+UWc4ZjYYGApsO+Y7NrYo6tRHISKSace86snd681sDvAKEAMed/f1ZrYQKHX3JcAcM7sYqAM+Aq6OVv8bYKGZ1QNx4Hp333vsWqlFISISirRuuHP3ZcCyFmXzk15/+yjrPQ883/ZaRUGhFoWISMaFOQ5GjloUIiKhCDMosnXVk4hIKMIMilgOWJbuoxARCUCYQWGWaFWoRSEiknFhBgUk+inUmS0iknHhBkV2vloUIiIBCDso1KIQEcm4cIMip4uGGRcRCUC4QZGdr2HGRUQCEG5Q5HTR5bEiIgEINyiy89SiEBEJQLhBkdNFndkiIgEINyhyC6D2YKZrISLS6QUcFN2g5pNM10JEpNMLNyjyukPtAfAjfnVVREROoXCDIrc7NNTrXgoRkQwLNyjyChLPtQcyWw8RkU4u3KDI7Z54VlCIiGRUwEHRLfFco6AQEcmkcIMiTy0KEZEQhBsUuVEfhVoUIiIZFW5QNLUodC+FiEgmhRsUjZ3ZalGIiGRU+EGhPgoRkYwKNyjyeySeqz/ObD1ERDq5cIMilpPo0K7am+maiIh0auEGBUDXM+CQgkJEJJPCDoouhVD1UaZrISLSqYUdFF0LdepJRCTDwg6KLjr1JCKSaYEHhU49iYhkWthB0bUQqvdDQzzTNRER6bTCDoouhYBD1b5M10REpNMKOyi69k48H6zMbD1ERDqxsIOix9mJ5092ZbYeIiKdWOBB0S/x/PHuzNZDRKQTCzsoCqIWxcdqUYiIZEpaQWFmM8xss5mVm9ncVuZfb2Z/MrMyM/tvMxuWNG9etN5mM5veptrl5Cc6tHXqSUQkY44ZFGYWAx4AZgLDgK8kB0HkF+5e7O4lwD3AvdG6w4ArgeHADOAn0fbS16O/WhQiIhmUTotiHFDu7tvcvRZ4Grg0eQF3Tx4LvBvg0etLgafdvcbdtwPl0fbS12sgfPRem1YREZH2k05Q9Ad2Jk1XRGXNmNmNZraVRIviprasm1Lvv4K9W3XTnYhIhqQTFNZKmR9R4P6Auw8BbgW+25Z1zew6Mys1s9LKyhb3TPQ5D+K1sE+tChGRTEgnKCqAgUnTA4BUnQZPA5e1ZV13f9jdx7j7mL59+zaf2Wdo4vnDLWlUVURE2ls6QbEGGGpmg8wsl0Tn9JLkBcxsaNLk54HGb/UlwJVmlmdmg4ChwOo21bDvpxLPf/lTm1YTEZH2kX2sBdy93szmAK8AMeBxd19vZguBUndfAswxs4uBOuAj4Opo3fVm9iywAagHbnT3tnU2dDkD+nwKdr7VptVERKR9HDMoANx9GbCsRdn8pNffTrHu3cDdx1tBAAaOg40vQUMDZIV9j6CIyOmmY3zrnjMRqvfBB+szXRMRkU6nYwTF0KlgWYlWhYiInFIdIyi6nwnnToJ3nk+cfhIRkVOmYwQFwMivwp5yKF+R6ZqIiHQqHScozr8cep4DK+6EeH2mayMi0ml0nKCI5cD0uxMd2q8uyHRtREQ6jbQujw3GsNkw5h/h9/cnOrc/dzvEOtZHEBHpaDret+zMe8Ab4PX7oPxVmHwzfGom5HTJdM1ERE5LHS8oYtkw6z4YMgV+cwcs/gfIzodzPwtDLoKB4+Gs4ZDbNdM1FRE5LXS8oGg07FL49Bdg26rElVDlr8Ly2xLzLAt6nZv4LYue50DPAdHrAZDXA3K7Q243yOsOOd10+kpEJIWO/Q2ZFYO/uijxANj/Z9hdBrvfTlxKu28nbH0VPvkLrYxunsQgKzuxPYtFz1mJ56zsI8taHT091ebbsnxA2xYRoaMHRUs9+ycen/588/L6Gvj4z4mfVK05ALWNj4OJR7wOGurB44kfSPKGxHRD/HBZ8us2SRVQLRdtw7Infdsi0nE4iYG+T47TKyiOJjsPCgcnHiIip6Mv/+ykbbrj3EchIiIZoaAQEZGUFBQiIpKSgkJERFJSUIiISEoKChERSUlBISIiKSkoREQkJfPA7tg1s0+AzZmuRyD6AB9muhKB0L44TPviMO2Lwz7l7gUnY8Mh3pm92d3HZLoSITCzUu2LBO2Lw7QvDtO+OMzMSk/WtnXqSUREUlJQiIhISiEGxcOZrkBAtC8O0744TPviMO2Lw07avgiuM1tERMISYotCREQCElRQmNkMM9tsZuVmNjfT9WlvZjbQzFaa2UYzW29m347KC83sN2a2JXo+Iyo3M7s/2h/rzGxU0raujpbfYmZXZ+oznSgzi5nZH83s5Wh6kJm9FX2uZ8wsNyrPi6bLo/lFSduYF5VvNrPpmfkkJ8bMepnZYjPbFB0fEzvrcWFm/yf6//GOmf3SzPI703FhZo+b2Qdm9k5SWbsdC2Y22sz+FK1zv1kaP5Pp7kE8gBiwFRgM5AJvA8MyXa92/oz9gFHR6wLgXWAYcA8wNyqfC3w/en0J8GsSv2E6AXgrKi8EtkXPZ0Svz8j05zvOfXIz8Avg5Wj6WeDK6PVDwLei1zcAD0WvrwSeiV4Pi46VPGBQdAzFMv25jmM/LAK+Eb3OBXp1xuMC6A9sB7okHQ/XdKbjAvgbYBTwTlJZux0LwGpgYrTOr4GZx6xTpndK0o6YCLySND0PmJfpep3kz/wiMJXEDYb9orJ+JO4lAfgp8JWk5TdH878C/DSpvNlyHeUBDABeBT4HvBwduB8C2S2PCeAVYGL0OjtazloeJ8nLdZQH0CP6crQW5Z3uuIiCYmf0BZcdHRfTO9txARS1CIp2ORaieZuSypstd7RHSKeeGg+QRhVR2WkpaiKPBN4CznL33QDR85nRYkfbJ6fLvroP+BegIZruDexz9/poOvlzNX3maP7+aPnTYV8MBiqBJ6LTcI+aWTc64XHh7n8G/h34H2A3iX/ntXTO4yJZex0L/aPXLctTCikoWjtPdlpekmVm3YHnge+4+8epFm2lzFOUdxhm9gXgA3dfm1zcyqJ+jHkdfl+Q+Et4FPCgu48EDpI4vXA0p+2+iM7UYzxMAAACAklEQVS9X0ridNHZQDdgZiuLdobjIh1t/fzHtV9CCooKYGDS9ABgV4bqctKYWQ6JkPi5u/9nVPy+mfWL5vcDPojKj7ZPTod9NQmYbWY7gKdJnH66D+hlZo1DyyR/rqbPHM3vCezl9NgXFUCFu78VTS8mERyd8bi4GNju7pXuXgf8J/BZOudxkay9joWK6HXL8pRCCoo1wNDo6oZcEh1TSzJcp3YVXV3wGLDR3e9NmrUEaLwq4WoSfReN5V+LrmyYAOyPmp2vANPM7IzoL7BpUVmH4e7z3H2AuxeR+Ld+zd3/DlgJfClarOW+aNxHX4qW96j8yujql0HAUBKddR2Gu/8F2Glmn4qKLgI20AmPCxKnnCaYWdfo/0vjvuh0x0UL7XIsRPM+MbMJ0f79WtK2ji7TnTYtOnAuIXEl0FbgtkzX5yR8vr8m0cxbB5RFj0tInFN9FdgSPRdGyxvwQLQ//gSMSdrW14Hy6PEPmf5sJ7hfLuTwVU+DSfyHLgeeA/Ki8vxoujyaPzhp/duifbSZNK7gCPEBlACl0bHxAokrVTrlcQEsADYB7wA/I3HlUqc5LoBfkuifqSPRAvjH9jwWgDHRvt0K/JgWF1G09tCd2SIiklJIp55ERCRACgoREUlJQSEiIikpKEREJCUFhYiIpKSgEBGRlBQUIiKSkoJCRERS+v8k70wXtETGswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "zkn =xgb.cv(xgb_params, xgb_train, num_boost_round=10000, nfold=10, stratified=False, folds=None, metrics=('rmse'), obj=None, feval=None, maximize=False, \n",
    "            fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=2019, callbacks=None, shuffle=True)\n",
    "zkn.loc[0:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n",
    "plt.savefig('aim2_rmse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.DMatrix at 0x1a224c1f28>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb.plot_importance(model_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist([np.round(test_pred_xgb),y_test],20)\n",
    "plt.legend(['Prediction','True value'])\n",
    "plt.savefig('aim2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds.\n",
      "[100]\ttraining's rmse: 0.365459\tvalid_1's rmse: 0.394734\n",
      "[200]\ttraining's rmse: 0.343647\tvalid_1's rmse: 0.377596\n",
      "[300]\ttraining's rmse: 0.333058\tvalid_1's rmse: 0.370135\n",
      "[400]\ttraining's rmse: 0.327828\tvalid_1's rmse: 0.366706\n",
      "Early stopping, best iteration is:\n",
      "[476]\ttraining's rmse: 0.32508\tvalid_1's rmse: 0.364917\n",
      "0.8128342245989305\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lgb_params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "#           'max_depth': 9,\n",
    "          'learning_rate': 0.004,\n",
    "#           'bagging_fraction': 0.85,\n",
    "#           'feature_fraction': 0.8,\n",
    "#           'min_split_gain': 0.02,\n",
    "#           'min_child_samples': 150,\n",
    "#           'min_child_weight': 0.02,\n",
    "#           'lambda_l2': 0.0475,\n",
    "#           'verbosity': -1,\n",
    "          'data_random_seed': 17\n",
    "    \n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "evals_result = {}\n",
    "gbm = lgb.train(lgb_params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=[lgb_train, lgb_test],\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=100,\n",
    "                early_stopping_rounds=20)\n",
    "test_pred_lgbm = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "print(accuracy_score(y_test, np.round(test_pred_lgbm).astype(np.int8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1fcd8b70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "zkn2 = lgb.cv(lgb_params,lgb_train,num_boost_round=10000,nfold=10)\n",
    "df = pd.DataFrame(data=zkn2)\n",
    "df.loc[0:,[\"rmse-mean\"]].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8FeXZ8PHfdU5WskA2FpNAIrIvIoRN0SoqoiLUulShFh+11Ed9bW1txb5qLT593tY+rdVqrdiqte0jWhFL1WoVtS5VICBr2CIECGsgEEgg67neP2YSDiHLIdtJzrm+n8/5zMw998y5bhKumdwzc4+oKsYYY8KDJ9gBGGOM6TiW9I0xJoxY0jfGmDBiSd8YY8KIJX1jjAkjlvSNMSaMWNI3xpgwYknfGGPCiCV9Y4wJIxHBDqC+1NRUzcrKCnYYxhjTpaxYseKAqqY1V6/TJf2srCxyc3ODHYYxxnQpIrI9kHrWvWOMMWEkoKQvIlNFZJOI5IvI3AbW3ywiRSKyyv3c5rdutohscT+z2zJ4Y4wxp6fZ7h0R8QJPAZcChcByEVmsqnn1qr6sqnfV2zYZ+DGQAyiwwt32UJtEb4wx5rQE0qc/DshX1a0AIrIAmAHUT/oNuQx4V1WL3W3fBaYCL7UsXGNMa1RVVVFYWEh5eXmwQzEtFBMTQ0ZGBpGRkS3aPpCknw7s9FsuBMY3UO8aEbkA2Azco6o7G9k2vf6GIjIHmAPQt2/fwCI3xpy2wsJCEhISyMrKQkSCHY45TarKwYMHKSwsJDs7u0X7CKRPv6HfjPpvXvk7kKWqI4H3gD+exrao6nxVzVHVnLS0Zu84Msa0UHl5OSkpKZbwuygRISUlpVV/qQWS9AuBTL/lDGC3fwVVPaiqFe7is8CYQLc1xnQsS/hdW2t/foEk/eXAABHJFpEo4AZgcb0g+vgtTgc2uPPvAFNEJElEkoApblnbO7wT3v8pFG9tl90bY0woaDbpq2o1cBdOst4AvKKq60VknohMd6vdLSLrRWQ1cDdws7ttMfAIzoFjOTCv9qJumysvgY8ehT2r22X3xpi2ER8f32ydrKwsDhw40AHRhJ+AnshV1beAt+qVPeQ3fz9wfyPbPgc814oYA5PUz5keKmj3rzLGGH/V1dVERHS6AQ4aFDpP5EYnQLcUOBTQk8jGmCDz+XzccccdDBs2jGnTpnHFFVfw6quv1q3/xS9+wbhx4xg3bhz5+fkN7iM+Pp777ruPMWPGcMkll7Bs2TIuvPBCzjzzTBYvdnqha2pq+MEPfsDYsWMZOXIkzzzzDAClpaVcfPHFjB49mhEjRvC3v/0NgIKCAoYMGcK3vvUthg0bxpQpUzh+/Pgp333zzTfzve99j4suuoj77ruPhx9+mNmzZzNlyhSysrJ47bXX+OEPf8iIESOYOnUqVVVVAMydO5ehQ4cycuRI7r33XgCKioq45pprGDt2LGPHjuXTTz9tu3/oerrGoSlQSVl2pm9MgH7y9/Xk7T7SpvscekYiP75qWEB1X3vtNQoKCli7di379+9nyJAh3HLLLXXrExMTWbZsGS+++CLf/e53eeONN07ZR1lZGRdeeCE///nPufrqq3nggQd49913ycvLY/bs2UyfPp0//OEPdO/eneXLl1NRUcF5553HlClTyMzMZNGiRSQmJnLgwAEmTJjA9OlOj/WWLVt46aWXePbZZ7n++utZuHAh3/jGN075/s2bN/Pee+/h9Xp5+OGH+fLLL/nggw/Iy8tj4sSJLFy4kEcffZSrr76aN998kwsuuIBFixaxceNGRITDhw8D8J3vfId77rmHSZMmsWPHDi677DI2bNhwyve1hdBL+rtWBjsKY0wAPvnkE6677jo8Hg+9e/fmoosuOmn9jTfeWDe95557GtxHVFQUU6dOBWDEiBFER0cTGRnJiBEjKCgoAOCf//wna9asqfsroqSkhC1btpCRkcGPfvQjPvroIzweD7t27WLfvn0AZGdnM2rUKADGjBlTt6/6rrvuOrxeb93y5ZdfXvf9NTU1J8VWUFDAtGnTiImJ4bbbbuPKK69k2rRpALz33nvk5Z143vXIkSMcPXqUhISEgP89AxV6ST/vb1BTDd7QapoxbS3QM/L2onrKIzsn8b81UUSoqalhzBjnbvDp06czb948IiMj6+p5PB6io6Pr5qurq+u+5ze/+Q2XXXbZSft/4YUXKCoqYsWKFURGRpKVlVV3/3vtfgC8Xm+D3TsAcXFxJy37f3/92Gr7/ZctW8aSJUtYsGABTz75JO+//z4+n4/PPvuM2NjYJv9N2kLo9OmDk/R91XBkV7AjMcY0Y9KkSSxcuBCfz8e+ffv48MMPT1r/8ssv100nTpyI1+tl1apVrFq1innz5gX8PZdddhlPP/10XZ/65s2bKSsro6SkhJ49exIZGckHH3zA9u3tfz2wtLSUkpISrrjiCn7961+zatUqAKZMmcKTTz5ZV6+2vD2E1ulwD787eGrv5jHGdErXXHMNS5YsYfjw4QwcOJDx48fTvXv3uvUVFRWMHz8en8/HSy+1fLiu2267jYKCAkaPHo2qkpaWxuuvv86sWbO46qqryMnJYdSoUQwePLgtmtWko0ePMmPGDMrLy1FVHnvsMQCeeOIJ7rzzTkaOHEl1dTUXXHABv/vd79olBmnuT6yOlpOToy1+icqh7fD4SLjqCRhjozgbU9+GDRsYMmRIsMOoU1paSnx8PAcPHmTcuHF8+umn9O7dO9hhdXoN/RxFZIWq5jS3bWid6SemgycCDtttm8Z0BdOmTePw4cNUVlby4IMPWsLvAKGV9L0R0D3Tbts0pouo349v2l9oXcgFpy/fkr4xxjQoBJN+liV9Y4xpRGgm/WMHobxtnzQ0xphQEJpJH+xirjHGNCB0k74NvGZMSLBhlttW6CX9HjbEsjEmeGqHf+isAkr6IjJVRDaJSL6IzG2i3rUioiKS4y5HisgfRWStiGwQkQbH3G9TsUkQ3d2SvjGd0PLlyxk5ciTl5eWUlZUxbNgw1q1bZ8Msd+Awy83epy8iXuAp4FKcd94uF5HFqppXr14CzluzlvoVXwdEq+oIEekG5InIS6pa0FYNaCBgu23TmED8Yy7sXdu2++w9Ai7/WaOrx44dy/Tp03nggQc4fvw43/jGNxg+fDivvvqqDbPcQcMsB/Jw1jggX1W3AojIAmAGkFev3iPAo8C9fmUKxIlIBBALVALtf1tNUhbsrx+eMaYzeOihhxg7diwxMTE88cQTgA2z3JHDLAeS9NOBnX7LhcB4/woicg6QqapviIh/0n8V5wCxB+gG3NNu78j1l9IfNr1lQywb05QmzsjbU3FxMaWlpVRVVVFeXk5cXJwNs9yBwywH0qcvDZTV/YRExAM8Bny/gXrjgBrgDCAb+L6InHnKF4jMEZFcEcktKioKKPAmJfd3hlgu2dH6fRlj2tScOXN45JFHmDVrFvfddx9gwyx35DDLgZwGFwKZfssZwG6/5QRgOPCheyTrDSwWkenATOBtVa0C9ovIp0AOsNX/C1R1PjAfnFE2W9YUPyn9nenBrZB8yjHGGBMkL774IhEREcycOZOamhrOPfdc3n//fRtmuQOHWW52aGW3P34zcDGwC1gOzFTV9Y3U/xC4V1VzReQ+YDBwC073znLgBlVd09j3tWpo5VpH98EvB8Llj8L4b7duX8aEkM42tLI/G2Y5cO06tLKqVovIXcA7gBd4TlXXi8g8IFdVFzex+VPA88A6nG6i55tK+G0mvidEJcDBhm/tMsZ0PjbMcscI6Cqnqr4FvFWv7KFG6l7oN1+Kc9tmxxKBlDPh4Jcd/tXGmJaxYZY7Rug9kVsruT8UW9I3pr7O9rY8c3pa+/ML3aSf0h8O74DqymBHYkynERMTw8GDBy3xd1GqysGDB4mJiWnxPkL3JvaUs0B9zmibqQOCHY0xnUJGRgaFhYW0ya3RJihiYmLIyMho8fahm/STa2/bzLekb4wrMjKS7OzsYIdhgii0u3fALuYaY4yf0E363ZIhpoddzDXGGD+hm/TB6de3M31jjKkT4km/PxRvbb6eMcaEidBO+sn9oWQnVDU8Kp4xxoSb0E76tRdzi7cFNw5jjOkkwiTpW7++McZAyCf9s5zpgc3BjcMYYzqJ0E760QmQmAFFm4IdiTHGdAqhnfQB0gZa0jfGGFfoJ/3UQXBgC/h8wY7EGGOCLqCkLyJTRWSTiOSLyNwm6l0rIioiOX5lI0XkMxFZLyJrRaTlw8O1RNogqCqDI4Ud+rXGGNMZNZv0RcSL8wasy4GhwI0iMrSBegnA3cBSv7II4M/A7ao6DLgQqGqTyAOVNsiZFtnFXGOMCeRMfxyQr6pbVbUSWADMaKDeI8CjQLlf2RRgjaquBlDVg6pa08qYT0+a+4Ljoo0d+rXGGNMZBZL004GdfsuFblkdETkHyFTVN+ptOxBQEXlHRFaKyA9bFW1LdEuGbqlwwC7mGmNMIOPpSwNlda/dEREP8BhwcyP7nwSMBY4BS9w3ti856QtE5gBzAPr27RtQ4KclbZB17xhjDIGd6RcCmX7LGcBuv+UEYDjwoYgUABOAxe7F3ELgX6p6QFWP4bxcfXT9L1DV+aqao6o5aWlpLWtJU9IGOd079oo4Y0yYCyTpLwcGiEi2iEQBNwCLa1eqaomqpqpqlqpmAZ8D01U1F3gHGCki3dyLul8B8tq8Fc1JHQTlh6HMXhFnjAlvzSZ9Va0G7sJJ4BuAV1R1vYjME5HpzWx7CPgVzoFjFbBSVd9sfdinqe4OHruYa4wJbwG9I1dV38LpmvEve6iRuhfWW/4zzm2bwVOX9DdB9gVBDcUYY4Ip9J/IBUjoA1EJNvCaMSbshUfSFzlxMdcYY8JYeCR9cB7SsoHXjDFhLnySfs8hULoPyg4EOxJjjAma8En6vYY5033rgxuHMcYEURgl/eHO1JK+MSaMhU/Sj0+DuJ6W9I0xYS18kj44XTz71gU7CmOMCZrwS/pFG6GmOtiRGGNMUIRZ0h8O1eVQvDXYkRhjTFCEWdKvvYPHuniMMeEpvJJ+2iAQr13MNcaErfBK+hHRkDrQkr4xJmyFV9IH9w4eS/rGmPAUnkm/ZAeUlwQ7EmOM6XBhmPRrn8zt+Bd4GWNMsAWU9EVkqohsEpF8EZnbRL1rRUTd9+P6l/cVkVIRube1Abea3cFjjAljzSZ9EfECTwGXA0OBG0VkaAP1EoC7gaUN7OYx4B+tC7WNJJ4B3VJgz6pgR2KMMR0ukDP9cUC+qm5V1UpgATCjgXqPAI8C5f6FIvJVYCvQOa6eikCfUbB7dbAjMcaYDhdI0k8HdvotF7pldUTkHCBTVd+oVx4H3Af8pKkvEJE5IpIrIrlFRUUBBd4qZ4yCog1QVd58XWOMCSGBJH1poEzrVop4cLpvvt9AvZ8Aj6lqaVNfoKrzVTVHVXPS0tICCKmV+owCX7XdummMCTsRAdQpBDL9ljOA3X7LCcBw4EMRAegNLBaR6cB44FoReRToAfhEpFxVn2yL4FvsjHOc6e6VkDEmqKEYY0xHCiTpLwcGiEg2sAu4AZhZu1JVS4DU2mUR+RC4V1VzgfP9yh8GSoOe8AG6Z9jFXGNMWGq2e0dVq4G7gHeADcArqrpeROa5Z/Ndj13MNcaEqUDO9FHVt4C36pU91EjdCxspf/g0Y2tfZ4yCTx93LuZGxgQ7GmOM6RDh90RuLbuYa4wJQ+Gb9M8Y5Uz3fBHcOIwxpgOFb9LvnulczN1tF3ONMeEjfJN+3cVcS/rGmPARvkkfnPv19+dB5bFgR2KMMR0ivJN+xljQGtht/frGmPBgSR9gZ0MDgxpjTOgJ76QflwIpA6BwebAjMcaYDhHeSR8gc5xzpq/afF1jjOniLOlnjoNjB6F4a7AjMcaYdmdJP3O8M7V+fWNMGLCknzoIorvDzmXBjsQYY9qdJX2PBzJyLOkbY8KCJX1wunj250F5SbAjMcaYdmVJHyBzLKCwa0WwIzHGmHYVUNIXkakisklE8kVkbhP1rhURFZEcd/lSEVkhImvd6eS2CrxNpecAAjs+D3YkxhjTrpp9iYqIeIGngEtx3pe7XEQWq2pevXoJwN2A/20wB4CrVHW3iAzHeftWelsF32ZiEqH3CNj+72BHYowx7SqQM/1xQL6qblXVSmABMKOBeo8AjwLltQWq+oWq1r5EfT0QIyLRrYy5fWRNcp7Mra4IdiTGGNNuAkn66cBOv+VC6p2ti8g5QKaqvtHEfq4BvlDVzplVsyZBdbn16xtjQlogSV8aKKsbs0BEPMBjwPcb3YHIMODnwLcbWT9HRHJFJLeoqCiAkNpB34mAQMGnwfl+Y4zpAIEk/UIg0285A9jtt5wADAc+FJECYAKw2O9ibgawCPimqn7Z0Beo6nxVzVHVnLS0tNNvRVvolgy9hsH2T4Lz/cYY0wECSfrLgQEiki0iUcANwOLalapaoqqpqpqlqlnA58B0Vc0VkR7Am8D9qtr5T6H7nec8pFVTFexIjDGmXTSb9FW1GrgL586bDcArqrpeROaJyPRmNr8LOAt4UERWuZ+erY66vWRNgqpj9lIVY0zIavaWTQBVfQt4q17ZQ43UvdBv/r+A/2pFfB2r33nOtOBjZ/RNY4wJMfZErr+4FOg1HL78INiRGGNMu7CkX1//yc6TuZVlwY7EGGPanCX9+vpPBl8VFNhdPMaY0GNJv76+EyEiFvKXBDsSY4xpc5b064uMgazz4Mv3gx2JMca0OUv6Del/MRzcAod3BDsSY4xpU5b0G9LfHQHazvaNMSEmZJJ+VY2P5z7ZRkV1Tet3ljYIEtOtX98YE3JCJukvLyhm3ht5/PebG1q/MxHofxFs/RfUVLd+f8YY00mETNI/t38q3zo/mz9+tp031uxufoPmnHUpVJTATnubljEmdIRM0gf44dTBjO7bg7kL17K1qLR1OzvrYvBGwca3mq9rjDFdREgl/UivhydnjibSK9zxl5WUV7Wifz86AbK/AhvfANXm6xtjTBcQUkkf4Iwesfzq66PYuPcoDy9e37qdDb4SDm+H/XnN1zXGmC4g5JI+wEWDenLnRf1ZsHwnC1cUtnxHgy53ptbFY4wJESGZ9AHuuWQg47OT+b+vr2XdrpKW7SShN6TnOF08xhgTAgJK+iIyVUQ2iUi+iMxtot61IqK1r0p0y+53t9skIpe1RdCBiHD795O6RfHtP63gYGkL38c++ErYswoO72y+rjHGdHLNJn0R8QJPAZcDQ4EbRWRoA/USgLuBpX5lQ3FerzgMmAr81t1fh0hLiGb+TTkcKK3gjr+spKrGd/o7GTrDmeb9rW2DM8aYIAjkTH8ckK+qW1W1ElgAzGig3iPAo0C5X9kMYIGqVqjqNiDf3V+HGZHRnZ9dM4Kl24r5aUse3ErpD33OhvWvtX1wxhjTwQJJ+umAf99GoVtWR0TOATJVtX7nd7PbdoSrz8ngtknZvPDvAl7JbUE3zbCvwa4VcKigzWMzxpiOFEjSlwbK6m5cFxEP8Bjw/dPd1m8fc0QkV0Ryi4qKAgjp9M29fDCTzkrlgUXrWLH90OltPOyrznT9orYPzBhjOlAgSb8QyPRbzgD8xzlIAIYDH4pIATABWOxezG1uWwBUdb6q5qhqTlpa2um1IEARXg+/ufEc+vSIYc6Luew4eCzwjZOyIGMsrH21XWIzxpiOEkjSXw4MEJFsEYnCuTC7uHalqpaoaqqqZqlqFvA5MF1Vc916N4hItIhkAwOAZW3eigAlxUXx3M1jqfYp//HCMkqOVQW+8civw751sHdt+wVojDHtrNmkr6rVwF3AO8AG4BVVXS8i80RkejPbrgdeAfKAt4E7VbUNxj5uuf5p8cy/aQw7io/x7T/nUlkd4B09w68BTySsXtC+ARpjTDsS7WTjyuTk5Ghubm67f8+iLwq55+XVfG10Or+87mxEGrr8UM+CWbBzGXxvA3gj2j1GY4wJlIisUNWc5uqF7BO5zbn6nAzuuWQgr63cxRNL8gPb6OwboWw/bP2gfYMzxph2ErZJH+Dui8/ia6PTeey9zYGN0TNgCsQmwxd/bv/gjDGmHYR10hcRfva1kZzbP4UfLlzDkg37mt4gIgpGzXTG4jnSBi9qMcaYDhbWSR8gKsLD/G/mMLRPInf8ZSXLC4qb3mDsbeCrgdznOyZAY4xpQ2Gf9AHioyN44T/Gkt4jllteWM6GPUcar5ycDQMvgxXPQ3ULB3EzxpggsaTvSomP5k+3jScuKoJvPres6Ye3xn0LyopsEDZjTJdjSd9Peo9Y/nTrOKpqfNz03FL2HSlvuOKZkyHlLFj6TMcGaIwxrWRJv54BvRJ4/uaxHDhawY3Pfk7R0Qa6cDweGPst2JXrDMRmjDFdhCX9BpzTN4kXbhnHnsPlzPr95w2/gGXUTIiKh2XPdnyAxhjTQpb0GzE2K5nnbh7LjuJjzPr9Ug6VVZ5cISYRzr4B1i2E0vYZGdQYY9qaJf0mTOyfwrPfzGHrgTJuem4pJcfrDdA2/naoqYLPnwpOgMYYc5os6Tfj/AFpPPONMWzae5RvPrfs5MSfOsAZa3/Zs3Csmfv7jTGmE7CkH4CLBvfkt7PGkLe7hJnPfk6xf1fPBT+AylK7k8cY0yVY0g/QpUN7Mf+bOeTvL+Xrz3zG/trbOXsNg8HTYOnTUN7EQ13GGNMJWNI/DRcN6skL/zGOXYePc90zn1F4yH2A64J7obwEls0PboDGGNMMS/qnaWL/FP5823iKyyq5/nefse1AGZxxDpx1KXz2FFSUBjtEY4xpVEBJX0SmisgmEckXkbkNrL9dRNaKyCoR+UREhrrlkSLyR3fdBhG5v60bEAyj+ybx0rcmUF7t4/pnPiNv9xH4yn1wvBj+/USwwzPGmEY1m/RFxAs8BVwODAVurE3qfv5XVUeo6ijgUeBXbvl1QLSqjgDGAN8Wkaw2ij2ohqd35+U5E4jwCNc/8xmfVmQ7r1T89HE4vCPY4RljTIMCOdMfB+Sr6lZVrQQWADP8K6iq/xXMOKD2HYwKxIlIBBALVAIhc7VzQK8EXrvjXNJ7xHLz88t454w7AIF/Phjs0IwxpkGBJP10YKffcqFbdhIRuVNEvsQ507/bLX4VKAP2ADuA/1HVU25oF5E5IpIrIrlFRV3r6dY+3WN55faJjO6bxLcX72N5xmzIex22fRzs0Iwx5hSBJP2G3hh+ytvUVfUpVe0P3Ac84BaPA2qAM4Bs4PsicmYD285X1RxVzUlLSws4+M6ie2wkL946jitH9uGmjeM5HNULffs+52UrxhjTiQSS9AuBTL/lDKCpdwUuAL7qzs8E3lbVKlXdD3wKNPu29q4oOsLLb244h1mTBvOj0q8j+9ZTvvQPwQ7LGGNOEkjSXw4MEJFsEYkCbgAW+1cQkQF+i1cCW9z5HcBkccQBE4CNrQ+7c/J4hAenDWXitFv43DeEqncepnDHtmCHZYwxdZpN+qpaDdwFvANsAF5R1fUiMk9EprvV7hKR9SKyCvgeMNstfwqIB9bhHDyeV9U1bd2Izuamc7OJmvEEkVSy5bnb+Hd+17pOYYwJXaJ6Svd8UOXk5Ghubm6ww2gTxe/+kuRP53FP9V2MmTaHb0zoF+yQjDEhSkRWqGqz3ef2RG47Sr74u1SfkcMjUX/k8dc/Zu7CNZRX2cVdY0zwWNJvTx4vEVc/TZy3hkWpz/Da8q1c8/S/m37pujHGtCNL+u0tbSDy1d+SUbqGD4b8nZ3FZVz5m495N29fsCMzxoQhS/odYdjVcMEPSN/2Kv/6yhb6pXTjWy/m8rN/bKS6xhfs6IwxYcSSfke58Ecw6AqSPvoxr11Wxazxffndv77k+mc+s+4eY0yHsaTfUTweuPoZSB1A1MKb+ekEH0/ceA5b9pdyxRMf89rKQjrbnVTGmNBjSb8jxSTCrL9CdAL86Wqm9znCP75zPkP7JPK9V1Zz94JVp7583Rhj2pAl/Y7Woy/MXgyeCPjjVWRUfMlLcyZw75SBvLV2D1c8/jH//vJAsKM0xoQoS/rBkNIfZr8B3kh4/kq8Oz/jrskDWPif5xLpFWY+u5QHXl9LaUV1sCM1xoQYS/rBkjYQbnkH4nvCn66GTf9gVGYP/vGdC7h1UjZ/WbqDyx77iI+32BAOxpi2Y0k/mHpkwi1vQ88hsGAWrPpfYqO8PDhtKK/ePpHoSA83/WEZcxeusb5+Y0ybsKQfbHGpMPvvkDUJXv9PePt+qKliTL9k3rr7fL79lTN5JXcnF//yXyz6wu7wMca0jiX9ziA6AWa9CuNvh89/Cy9cCUd2ExPp5f7Lh/C3OyeRnhTLPS+v5ob5n7N539FgR2yM6aIs6XcWEVFw+c/h2udg7zr43fmw5V0ARmR0Z9F/nst/Xz2CTfuOcsXjH/PTN/PsQq8x5rRZ0u9shl8Dcz6AuDT4y7Ww6HYoLcLjEWaO78v737+Qa8dk8OzH27j4lx/y99W7rcvHGBOwgMbTF5GpwOOAF/i9qv6s3vrbgTtx3odbCsxR1Tx33UjgGSAR8AFjVbW8se8KpfH0W6W6Aj76BXzyGETFweQHIecW8HgBWLnjEA++vo71u48wpl8SP7piMGP6JQc5aBNWVKGmCnxV7rTamdZUnpivXVc3X3mirq8GtMaZ+mqcMvWf952oV7fO5zdf7cQAILWv8hZnXrzO/xXxuPPuVDx+9eXU7WpfCX7Ker8y1PleX40To/rcMp9TXrfsfvzXNVrPne81As7+eot+HIGOp99s0hcRL7AZuBTnfbnLgRtrk7pbJ1FVj7jz04E7VHWqiEQAK4GbVHW1iKQAh1W10UHlLenXU7QZ/vED2PohpA2Bix+EQVeACDU+5a+5O/nVu5vZf7SCqcN6c9/lg8lOjQt21KazUYXKMqg46vc5Um+5gbKqY852Vceg8pgzrTp2IrEHiyfCL4nriTaCm1TdhNwpiXswkpPnxQNDZ8DVv2vZXgNM+hEB7GsckK+qW90dLwBmAHVJvzbhu+Ko+ykwBVijqqvdegcDC9/USRsIN70OGxbDknmwYCZkjIXzvot30OXcMK4v00edwbMfbeOZj77kvQ37mDW+L3efh8GjAAAQG0lEQVRfPICU+OhgR2/aQnWlk4zLS6D8sJOMy4+cSNC18+Ul9RJ36cnJnAC6ASNinBsLohMgKt75xCZB93SIjIOobhDZDbxRzsOF3kjw+E8j3GmU33ztJ8r5eCKds3CP90Tyrl0Wt6xunadevQjnrD1QPt+Jvx60xu9sWzn5YKEnDhr+ZY2tr/srojZpe2gymdf9BeL3l0OQBJL004GdfsuFwPj6lUTkTpz340YBk93igYCKyDtAGrBAVR9tVcThSMQ5Axh0Jaz6C3z0P/DyLEjMgBHX0G3k1/nOJcOYOb4vv35vM39euoOFK3dxy3lZ3DrpTLp3iwx2C8JbTZWTmMsPu4m7oU8T66oCGIU1Ms5J1DGJbtJOhIQ+zrQ2iZ/0SYTo+JOXo+KdGwpCiccDeJyDjgEC6965DrhMVW9zl28Cxqnq/2mk/ky3/mwRuRenr38scAxYAjygqkvqbTMHmAPQt2/fMdu3b29dq0JdTTVs/Dus+l/IX+KcwfQcBiOuhf6Tyfdk88v38vnHur0kREfwH5OyuXVSNt1j7Re/RVSdM+Vjxc7nuDv17wqpLD01WR93E3lVWdP7Fw/EdIeYHu60/qe2PNGZ1ibpmEQ3eSc6Z9UmrLVln/5E4GFVvcxdvh9AVf9fI/U9wCFV7S4iNwBTVfVmd92DQLmq/qKx77M+/dNUdgDWL4I1L0Phcqcsujv0O5e9yWN4rjCD3+fHExcTxa2TsrllUjaJMWGc/H0+56z6+CE4dtBN5AfdRO6/fOjkdU31X3si3G6QhpJ2Y4nc7xMV3yn+7DddW1sm/QicC7kXA7twLuTOVNX1fnUGqOoWd/4q4MeqmiMiSThn95OASuBt4DFVfbOx77Ok3wpHdkPBp7D9Eyj4BA7mA1ATmcD6yKG8XdKPnRFZDBs1lq9ddC49e8QHOeAWUD31omNd//YRd95dd/zQyZ9jxU7Cb+wCnycCuqVAbDJ0cz+xyU5Zt2S/de5ybRdJRIwlbRN0bXYhV1WrReQu4B2cWzafU9X1IjIPyFXVxcBdInIJUAUcAma72x4SkV/hHCgUeKuphG9aKfEMGHmd8wE4sge2f4q34GNGFnzCyGNLnfLVULnKy76YDOL6DCK+ZzZ0z3DGAkroA/G9nKTmf5Gt9ra3lvD5nC6OyjLn4mJl7ce9m6TSXVdbXlFab/mom7jd7pLGb/5yidMFEtvDuQgZmwzdM935JDdpp5ya2KMTLHmbkBfQffodyc7021H5ESjayIGCteStXUnF3k1kspe+EcV08zXT71yr9ja5ugNBQ8teJ3lWHT9xi1+gImKc5xJq7xyJjvfrOnGTeG3/dm1/dv0LmFHxLT9AGdNFtVn3TkezpN9xDpRW8OK/C3jx8+3UHCthYuoxrh8UwaTe1cRUHfF7eKbebW+BlKN+Cbw2ibt3mJyS1P3W210WxrSIJX0TsOOVNfx9zW7+9Nl21u4qIT46gq+NTuemCf0Y0Csh2OEZYwJgSd+cNlVl1c7D/Omz7byxZg+VNT4mnJnMNydmcenQXkR6rcvEmM7Kkr5plYOlFbySW8ifP9/OrsPHSY2P4quj0rk2J4PBvRODHZ4xph5L+qZN1PiUDzft56+5hSzZuI+qGmV4eiLXjs5gxqh0kuJC7AlOY7ooS/qmzRWXVbJ41S5eXVnIul1HiPQK5w9I46qz+3DJkF4khPNDX8YEmSV906427DnCoi928eaaPew6fJyoCA+TB/Vk2tl9uHhwL2KjvMEO0ZiwYknfdAifT/li5yH+vnoPb67dQ9HRCmIjvUwe3JNLhvbkokE96dHNuoCMaW+W9E2Hq/Epy7YV88aa3fwzbx9FRyvweoScfklcOrQXlw7tRb8UG+vfmPZgSd8Elc+nrNlVwnt5+3hvwz427nVe5j6gZzyTB/fk/AFp5GQlERNp3UDGtAVL+qZT2Vl8jPc27OPdvH0sLyimqkaJjvAw/swUzj8rlfMHpjKoVwJiY98Y0yKW9E2nVVZRzdJtB/l4ywE+3nKA/P2lAKQlRHP+Wamcd1Yq47KTyUiKtYOAMQFqy9clGtOm4qIjmDy4F5MH9wJgT8lxPt5ygE+2HODDzUW89sUuAHolRjM2K7nuM6h3Al6PHQSMaQ070zedis+nbN5/lOUFh1i+rZjlBcXsKSkHICE6gtH9khiXncyYfkmMSO9OXLSdtxgD1r1jQkjhoWPkFhxiWUExuQXFbN7ndAd5BM7qGc+I9B6cndmdkRk9GNIngegIuzhswo8lfROyDpVV8sXOQ6zeWcKawsOsKSzhYFklAJFeYVDvBEZm9GD4Gd0Z1DuBQb0TiLe/CEyIa9OkLyJTgcdx3pz1e1X9Wb31t+O8AL0GKAXmqGqe3/q+QB7Ou3b/p6nvsqRvTpeqsruknDU7D7O60DkQrC0s4WjFiffaZibHMrh3IoPdg8Dg3olkpXQjwkYONSGiLd+R68V5R+6lQCHOqw9vrJfUE1X1iDs/HbhDVaf6rV8I+ICllvRNR/D5lF2Hj7NhzxE27T3Kxn1H2bjnCNsOlOFzf+WjIjwM7BXPoF7OwaB/zziyU+PJTIq1g4Hpctry7p1xQL6qbnV3vACYgXPmDkBtwnfF4bwPtzaQrwJbgQDfx2dM63k8QmZyNzKTuzFlWO+68vKqGvL3l7Jx71E27T3Cxr1H+WhLEQtXFtbVifAIfVO6cWZqHNmpzoEgOzWOM9Pi6JkQbbeRmi4tkKSfDuz0Wy4ExtevJCJ3At8DooDJblkccB/OXwn3NvYFIjIHmAPQt2/fAEM35vTFRHoZnt6d4endTyovLqtk24FSthaVse3Aic/HWw5QUe2rqxcb6SUjKZaMpFgyk7s506RuZCQ58z26RdpBwXRqgST9hn6DT+kTUtWngKdEZCbwADAb+AnwmKqWNvUfQVXnA/PB6d4JICZj2lRyXBTJccmM6Zd8UrnPp+w5Us62ojK2Hiil4MAxCg8do/DQcXK3H+JoefVJ9eOjI+oOCr0SY9xPtN98DEl2YDBBFEjSLwQy/ZYzgN1N1F8APO3OjweuFZFHgR6AT0TKVfXJlgRrTEfzeIT0HrGk94hl0oDUU9aXHK9iZ7FzEKg9GNROV+44TLF7V5G/KK+HnonR9HYPAo3N2zMIpj0E8lu1HBggItnALuAGYKZ/BREZoKpb3MUrgS0Aqnq+X52HgVJL+CaUdI+NpHsD3UW1Kqpr2H+kgn1HytlXNy2vW96w5wgfbCrnWGXNKdvGR0ec8ldCanwUSd2iSI6LIikuiuRuUSTFRRIfHWF/PZiANJv0VbVaRO4C3sG5ZfM5VV0vIvOAXFVdDNwlIpcAVcAhnK4dY8JedIS37oJyU0orqtlbUs7+I+XsbeAAsWxbMfuPllNV03DvZ6RXThwM3OmJA0MkCTGRxEVHEBftdaZR7nxUBHHREURF2N1K4cIezjKmi1BVjlZUc6iskuKySg4dq6S4rMpZPlbJobJKDpZVnrR8+HgVgfwXj/TKSQeDblERxEdH0C3KOUhER3iIivAQ5XWmke60oXKvR/CI4PUIXg8n5kXweOTk9SJ4PNTNe+uvb6RubZng7F+EsP9LxwZcMybEiAiJMZEkxkQG/DKaGp9y+FglpRXVlFZUc6yyxplW1FBWUU1ZZbU7dZfrlR8oraC0oprKah+VNT5nWu2j2te5ThZribgHAZwp4gzXIchJ62oPEh6/KdQu+9erPaDU+55697ecur5+XNLoOv+Ciwb15MFpQ0+32afFkr4xIczrEVLio0mJj27T/fp86hwE/A4EldU+qmp81KhS41N8Pk7M15VpvTJOWl/30RN1fXVlnLy9T1FAFXxaO68nLftUoXZZwaegOPPqV6d2HThxK+rUdev5q3+4a3594Num94g9nR9Di1jSN8acNo9HiPF47c1nXZBdvTHGmDBiSd8YY8KIJX1jjAkjlvSNMSaMWNI3xpgwYknfGGPCiCV9Y4wJI5b0jTEmjHS6sXdEpAjY3opdpAIH2iicYAqVdoC1pTMKlXaAtaVWP1VNa65Sp0v6rSUiuYEMOtTZhUo7wNrSGYVKO8Dacrqse8cYY8KIJX1jjAkjoZj05wc7gDYSKu0Aa0tnFCrtAGvLaQm5Pn1jjDGNC8UzfWOMMY0ImaQvIlNFZJOI5IvI3GDH0xwReU5E9ovIOr+yZBF5V0S2uNMkt1xE5Am3bWtEZHTwIj+ZiGSKyAciskFE1ovId9zyrtiWGBFZJiKr3bb8xC3PFpGlblteFpEotzzaXc5312cFM/6GiIhXRL4QkTfc5S7ZFhEpEJG1IrJKRHLdsq74O9ZDRF4VkY3u/5mJHd2OkEj6IuIFngIuB4YCN4pI+75zrPVeAKbWK5sLLFHVAcASdxmcdg1wP3OApzsoxkBUA99X1SHABOBO99++K7alApisqmcDo4CpIjIB+DnwmNuWQ8Ctbv1bgUOqehbwmFuvs/kOsMFvuSu35SJVHeV3S2NX/B17HHhbVQcDZ+P8bDq2Hc7rxbr2B5gIvOO3fD9wf7DjCiDuLGCd3/ImoI873wfY5M4/A9zYUL3O9gH+Blza1dsCdANWAuNxHpaJqP+7BrwDTHTnI9x6EuzY/dqQgZNEJgNv4LyNtau2pQBIrVfWpX7HgERgW/1/145uR0ic6QPpwE6/5UK3rKvppap7ANxpT7e8S7TP7RI4B1hKF22L2x2yCtgPvAt8CRxW1Wq3in+8dW1x15cAKR0bcZN+DfwQ8LnLKXTdtijwTxFZISJz3LKu9jt2JlAEPO92uf1eROLo4HaEStI/5QXznPoO4q6s07dPROKBhcB3VfVIU1UbKOs0bVHVGlUdhXOWPA4Y0lA1d9pp2yIi04D9qrrCv7iBqp2+La7zVHU0TpfHnSJyQRN1O2tbIoDRwNOqeg5QxomunIa0SztCJekXApl+yxnA7iDF0hr7RKQPgDvd75Z36vaJSCROwv+Lqr7mFnfJttRS1cPAhzjXKXqISIS7yj/eura467sDxR0baaPOA6aLSAGwAKeL59d0zbagqrvd6X5gEc4Buav9jhUChaq61F1+Fecg0KHtCJWkvxwY4N6ZEAXcACwOckwtsRiY7c7Pxukfry3/pns1fwJQUvvnYLCJiAB/ADao6q/8VnXFtqSJSA93Pha4BOdC2wfAtW61+m2pbeO1wPvqdr4Gm6rer6oZqpqF8//hfVWdRRdsi4jEiUhC7TwwBVhHF/sdU9W9wE4RGeQWXQzk0dHtCPbFjTa8SHIFsBmnD/b/BjueAOJ9CdgDVOEc0W/F6UNdAmxxp8luXcG5O+lLYC2QE+z4/doxCedPzjXAKvdzRRdty0jgC7ct64CH3PIzgWVAPvBXINotj3GX8931Zwa7DY2060Lgja7aFjfm1e5nfe3/7y76OzYKyHV/x14Hkjq6HfZErjHGhJFQ6d4xxhgTAEv6xhgTRizpG2NMGLGkb4wxYcSSvjHGhBFL+sYYE0Ys6RtjTBixpG+MMWHk/wNnZh9uCnDjFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df.loc[0:600,[\"rmse-mean\"]],label='lgb-mean rmse')\n",
    "plt.plot(zkn.loc[0:600,[\"test-rmse-mean\"]],label='xgb-mean rmse')\n",
    "plt.legend()\n",
    "plt.savefig('kfold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CatB:\n",
      "0:\tlearn: 0.8804808\ttotal: 60.2ms\tremaining: 20m 4s\n",
      "1000:\tlearn: 0.3156558\ttotal: 3.75s\tremaining: 1m 11s\n",
      "2000:\tlearn: 0.3111874\ttotal: 7s\tremaining: 1m 2s\n",
      "3000:\tlearn: 0.3078004\ttotal: 10s\tremaining: 56.8s\n",
      "4000:\tlearn: 0.3060599\ttotal: 13.3s\tremaining: 53.4s\n",
      "5000:\tlearn: 0.3044311\ttotal: 16.3s\tremaining: 48.7s\n",
      "6000:\tlearn: 0.3031389\ttotal: 20s\tremaining: 46.6s\n",
      "7000:\tlearn: 0.3016471\ttotal: 23s\tremaining: 42.7s\n",
      "8000:\tlearn: 0.3007427\ttotal: 26.5s\tremaining: 39.7s\n",
      "9000:\tlearn: 0.3000043\ttotal: 29.4s\tremaining: 36s\n",
      "10000:\tlearn: 0.2995094\ttotal: 32.9s\tremaining: 32.9s\n",
      "11000:\tlearn: 0.2990461\ttotal: 36s\tremaining: 29.5s\n",
      "12000:\tlearn: 0.2987102\ttotal: 39.3s\tremaining: 26.2s\n",
      "13000:\tlearn: 0.2982654\ttotal: 42.2s\tremaining: 22.7s\n",
      "14000:\tlearn: 0.2979313\ttotal: 45.6s\tremaining: 19.5s\n",
      "15000:\tlearn: 0.2976762\ttotal: 49s\tremaining: 16.3s\n",
      "16000:\tlearn: 0.2974121\ttotal: 52.1s\tremaining: 13s\n",
      "17000:\tlearn: 0.2971005\ttotal: 55.5s\tremaining: 9.79s\n",
      "18000:\tlearn: 0.2968512\ttotal: 59.8s\tremaining: 6.64s\n",
      "19000:\tlearn: 0.2966006\ttotal: 1m 3s\tremaining: 3.31s\n",
      "19999:\tlearn: 0.2963784\ttotal: 1m 6s\tremaining: 0us\n",
      "0.786096256684492\n"
     ]
    }
   ],
   "source": [
    "from catboost import Pool, CatBoostRegressor\n",
    "cat_params = {'depth': 8,\n",
    "              'eta': 0.01,\n",
    "#               'random_strength': 1.5,\n",
    "#               'one_hot_max_size': 2,\n",
    "#               'reg_lambda': 6,\n",
    "              'od_type': 'Iter',\n",
    "#               'fold_len_multiplier': 2,\n",
    "              'bootstrap_type': \"Bayesian\",\n",
    "#               'bagging_temperature': 1,\n",
    "              'random_seed': 1337,\n",
    "              # 'verbose_eval': 100,\n",
    "#               'early_stopping_rounds': 500,\n",
    "              'num_boost_round': 20000}\n",
    "\n",
    "\n",
    "foo = X_train.dtypes\n",
    "cat_feature_names = foo[foo == \"category\"]\n",
    "cat_features = [X_train.columns.get_loc(c) for c in X_train.columns if c in cat_feature_names]\n",
    "# watchlist3 = Pool(X_train, y_train)\n",
    "print('training CatB:')\n",
    "\n",
    "model3 = CatBoostRegressor(cat_features=list(cat_features), **cat_params)\n",
    "# train the model\n",
    "model3.fit(X_train, y_train,verbose=1000)\n",
    "# make the prediction using the resulting model\n",
    "test_pred3 = model3.predict(X_test)\n",
    "print(accuracy_score(y_test, np.round(test_pred3).astype(np.int8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8128342245989305"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    " \n",
    "clf = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "result = clf.predict(X_test) \n",
    "\n",
    "accuracy_score(result,y_test)\n",
    "# print(clf.predict([[-0.8, -1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "Epoch 1/50\n",
      "446/446 [==============================] - 0s 607us/sample - loss: 0.5588 - acc: 0.7848\n",
      "Epoch 2/50\n",
      "446/446 [==============================] - 0s 15us/sample - loss: 0.5655 - acc: 0.7848\n",
      "Epoch 3/50\n",
      "446/446 [==============================] - 0s 15us/sample - loss: 0.5638 - acc: 0.7870\n",
      "Epoch 4/50\n",
      "446/446 [==============================] - 0s 18us/sample - loss: 0.5861 - acc: 0.7870\n",
      "Epoch 5/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5458 - acc: 0.7870\n",
      "Epoch 6/50\n",
      "446/446 [==============================] - 0s 16us/sample - loss: 0.5453 - acc: 0.7870\n",
      "Epoch 7/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5505 - acc: 0.7870\n",
      "Epoch 8/50\n",
      "446/446 [==============================] - 0s 18us/sample - loss: 0.5599 - acc: 0.7870\n",
      "Epoch 9/50\n",
      "446/446 [==============================] - 0s 21us/sample - loss: 0.5377 - acc: 0.7870\n",
      "Epoch 10/50\n",
      "446/446 [==============================] - 0s 18us/sample - loss: 0.5470 - acc: 0.7870\n",
      "Epoch 11/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5392 - acc: 0.7870\n",
      "Epoch 12/50\n",
      "446/446 [==============================] - 0s 22us/sample - loss: 0.5476 - acc: 0.7870\n",
      "Epoch 13/50\n",
      "446/446 [==============================] - 0s 18us/sample - loss: 0.5610 - acc: 0.7870\n",
      "Epoch 14/50\n",
      "446/446 [==============================] - 0s 18us/sample - loss: 0.5341 - acc: 0.7870\n",
      "Epoch 15/50\n",
      "446/446 [==============================] - 0s 16us/sample - loss: 0.5727 - acc: 0.7848\n",
      "Epoch 16/50\n",
      "446/446 [==============================] - 0s 18us/sample - loss: 0.5421 - acc: 0.7870\n",
      "Epoch 17/50\n",
      "446/446 [==============================] - 0s 18us/sample - loss: 0.5450 - acc: 0.7892\n",
      "Epoch 18/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5543 - acc: 0.7870\n",
      "Epoch 19/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5427 - acc: 0.7892\n",
      "Epoch 20/50\n",
      "446/446 [==============================] - 0s 15us/sample - loss: 0.5477 - acc: 0.7870\n",
      "Epoch 21/50\n",
      "446/446 [==============================] - 0s 18us/sample - loss: 0.5434 - acc: 0.7870\n",
      "Epoch 22/50\n",
      "446/446 [==============================] - 0s 20us/sample - loss: 0.5439 - acc: 0.7870\n",
      "Epoch 23/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5414 - acc: 0.7870\n",
      "Epoch 24/50\n",
      "446/446 [==============================] - 0s 16us/sample - loss: 0.5371 - acc: 0.7870\n",
      "Epoch 25/50\n",
      "446/446 [==============================] - 0s 20us/sample - loss: 0.5427 - acc: 0.7870\n",
      "Epoch 26/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5610 - acc: 0.7870\n",
      "Epoch 27/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5425 - acc: 0.7870\n",
      "Epoch 28/50\n",
      "446/446 [==============================] - 0s 20us/sample - loss: 0.5353 - acc: 0.7870\n",
      "Epoch 29/50\n",
      "446/446 [==============================] - 0s 16us/sample - loss: 0.5386 - acc: 0.7870\n",
      "Epoch 30/50\n",
      "446/446 [==============================] - 0s 22us/sample - loss: 0.5315 - acc: 0.7870\n",
      "Epoch 31/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5374 - acc: 0.7870\n",
      "Epoch 32/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5471 - acc: 0.7870\n",
      "Epoch 33/50\n",
      "446/446 [==============================] - 0s 20us/sample - loss: 0.5429 - acc: 0.7870\n",
      "Epoch 34/50\n",
      "446/446 [==============================] - 0s 18us/sample - loss: 0.5530 - acc: 0.7870\n",
      "Epoch 35/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5187 - acc: 0.7870\n",
      "Epoch 36/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5394 - acc: 0.7870\n",
      "Epoch 37/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5447 - acc: 0.7870\n",
      "Epoch 38/50\n",
      "446/446 [==============================] - 0s 20us/sample - loss: 0.5330 - acc: 0.7870\n",
      "Epoch 39/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5392 - acc: 0.7870\n",
      "Epoch 40/50\n",
      "446/446 [==============================] - 0s 18us/sample - loss: 0.5288 - acc: 0.7870\n",
      "Epoch 41/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5473 - acc: 0.7870\n",
      "Epoch 42/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5408 - acc: 0.7870\n",
      "Epoch 43/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5493 - acc: 0.7870\n",
      "Epoch 44/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5560 - acc: 0.7870\n",
      "Epoch 45/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5458 - acc: 0.7870\n",
      "Epoch 46/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5450 - acc: 0.7870\n",
      "Epoch 47/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5407 - acc: 0.7870\n",
      "Epoch 48/50\n",
      "446/446 [==============================] - 0s 16us/sample - loss: 0.5412 - acc: 0.7870\n",
      "Epoch 49/50\n",
      "446/446 [==============================] - 0s 19us/sample - loss: 0.5287 - acc: 0.7870\n",
      "Epoch 50/50\n",
      "446/446 [==============================] - 0s 17us/sample - loss: 0.5431 - acc: 0.7870\n",
      "187/187 [==============================] - 0s 748us/sample - loss: 0.5583 - acc: 0.7540\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "train_dataset = Xtrain\n",
    "test_dataset = Xtest\n",
    "train_labels = ytrain\n",
    "test_labels = ytest\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = Xtrain\n",
    "y_train = ytrain\n",
    "x_test = Xtest\n",
    "y_test = ytest\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=8, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=50,\n",
    "          batch_size=128)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(X_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = np.ones((446),dtype=np.int16)\n",
    "Xtrain = np.ones((446,8),dtype=np.int16)\n",
    "\n",
    "for i in range(446):\n",
    "    X_train\n",
    "    ytrain[i] = y_train[i]\n",
    "    Xtrain[i] = X_train.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)\n",
    "ytest = np.ones((187),dtype=np.int16)\n",
    "Xtest = np.ones((187,8),dtype=np.int16)\n",
    "for i in range(187):\n",
    "    ytest[i] = y_test[i]\n",
    "    Xtest[i] = X_test.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2, ..., 2, 0, 1],\n",
       "       [1, 0, 2, ..., 2, 0, 1],\n",
       "       [1, 0, 2, ..., 2, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 2, ..., 2, 0, 1],\n",
       "       [1, 0, 2, ..., 2, 0, 1],\n",
       "       [1, 0, 2, ..., 2, 0, 1]], dtype=int16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
